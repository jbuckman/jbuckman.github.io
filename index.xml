<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Buckman&#39;s Homepage on Buckman&#39;s Homepage</title>
    <link>https://jacobbuckman.com/</link>
    <description>Recent content in Buckman&#39;s Homepage on Buckman&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 16 May 2019 18:38:24 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>On Peer Review</title>
      <link>https://jacobbuckman.com/post/on-peer-review/</link>
      <pubDate>Thu, 16 May 2019 18:38:24 -0400</pubDate>
      
      <guid>https://jacobbuckman.com/post/on-peer-review/</guid>
      <description>

&lt;p&gt;In light of the &lt;a href=&#34;https://twitter.com/nlpmattg/status/1128319419376066560&#34; target=&#34;_blank&#34;&gt;recent discussions&lt;/a&gt; on the *ACL reviewing process on Twitter, I want to share some thoughts.&lt;/p&gt;

&lt;h3 id=&#34;do-we-need-peer-review&#34;&gt;Do We Need Peer Review?&lt;/h3&gt;

&lt;p&gt;Specifically, do we need double-blind peer review of the sort that conferences provide?&lt;/p&gt;

&lt;p&gt;I’m in full agreement with Ryan that it is an essential service for the scientific community. As scientists, our job is to develop and capture knowledge. Peer review ensures that the work of the least-advantaged members of our community is judged by the same standards as the most-advantaged members. By “advantage”, I mean any number of intangible qualities that might cause you to trust a researcher, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Being a well-known senior name in the field&lt;/li&gt;
&lt;li&gt;Coming from a respected institution or group&lt;/li&gt;
&lt;li&gt;Having significant funding for PR&lt;/li&gt;
&lt;li&gt;Being a member of a privileged racial group&lt;/li&gt;
&lt;li&gt;Charisma&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Peer review is an invaluable resource for &lt;em&gt;disadvantaged&lt;/em&gt; researchers, who lack the above qualities. In reality, of course, being “advantaged” or “disadvantaged” is not a boolean, or even a scalar, but hopefully it’s a coherent enough concept to get the point across. I think it’s fair to say that in general, the more disadvantaged a researcher is, the more they are forced to rely on the peer review process to build their resume and share their work with the community.&lt;/p&gt;

&lt;p&gt;In fact, I see this as the &lt;em&gt;main&lt;/em&gt; benefit of the double-blind peer review system. Given almost any objective function other than “ensure fair treatment of the ideas of researchers who lack advantages”, and peer review is an absolutely abysmal way of doing things. Any proposed changes to the peer review system must therefore be centered around achieving this objective. The only acceptable alternatives are those for which the opportunities for disadvantaged researchers are equal or better to their opportunities under double-blind peer review.&lt;/p&gt;

&lt;h3 id=&#34;the-goal-of-a-conference&#34;&gt;The Goal of a Conference&lt;/h3&gt;

&lt;p&gt;Before suggesting changes, let’s take a closer look at what conferences&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:0&#34;&gt;&lt;a href=&#34;#fn:0&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; actually set out to accomplish. I would argue that it is a mix of three elements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Feedback.&lt;/em&gt; By soliciting detailed comments from knowledgeable reviewers, peer review can help authors catch their mistakes and guide them towards doing better science.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Certification.&lt;/em&gt; When a paper is accepted to a conference or journal, the community is in some sense acknowledging the work (and its authors) as having merit.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Dissemination.&lt;/em&gt; When a top conference releases is proceedings, many researchers skim the proceedings and read papers they find interesting. At the conference itself, a poster or oral presentation is another chance to share your ideas directly with peers, who might build on your work.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the pre-Internet era, a conference or journal accomplished all three goals, and was basically the only avenue for any of them. Nowadays, though, feedback and dissemination are easy to come by; OpenReview and arXiv provide ample opportunities for review and feedback, and a post to Twitter, a Reddit thread, or a podcast appearance can easily get your work into the eyes/ears of hundreds of your peers. Therefore, the focus of conferences has shifted, to become centered around the remaining component: certification.&lt;/p&gt;

&lt;p&gt;The “certification effect” of a conference acceptance has become so massive as to be almost the entire point of conferences. Entire careers are can be put on hold on the basis of an unlucky conference rejection for a borderline paper. Think about what your peers complain about when they get rejected. How many times have you heard someone say, “I’m mad at the reviewers - they totally understood and loved my paper, yet weren’t able to give me any useful comments on how to improve it”? I’d wager almost never. The disappointment just isn’t related to feedback or dissemination. The vast majority of complaints about peer review revolve, explicitly or not, around certification: “With this unfair paper rejection, I’m one step further from grad school acceptance/graduation/a job/tenure/etc.”&lt;/p&gt;

&lt;p&gt;Certification from a conference also enables disadvantaged researchers to take advantage of the aforementioned Internet feedback and dissemination techniques. There’s so much content on the Internet that it’s impossible to consume it all, and high-advantage researchers have a stark advantage in the attention economy. A conference’s certification carries a lot of weight: a tweet saying “Check out my new work, selected for a best paper award at ICLR!” is going to get clicks and retweets regardless who is posting.&lt;/p&gt;

&lt;p&gt;Therefore, I argue that the main benefit of double-blind peer review is to effectively and unbiasedly certify papers as high-quality research, and people as high-quality researchers. All other objectives are secondary. It is essential that we keep this in mind when discussing alternatives and improvements to peer review.&lt;/p&gt;

&lt;h3 id=&#34;the-problem-with-peer-review&#34;&gt;The Problem with Peer Review&lt;/h3&gt;

&lt;p&gt;I think most people would agree that peer review does quite a good job at evaluating papers unbiasedly. The main complaint, especially in the ML community nowadays, is that peer review is too high-variance. While the top 10% of papers are reliably accepted, and the bottom 40% are reliably rejected, the middle 50% of papers often feel like a coin flip. This is extremely frustrating for people who rely on conference acceptances to boost their careers, such as undergrad/master’s students who are trying to get into grad school, and PhD students who are trying to graduate. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The widely-acknowledged reason for this is that there are simply an enormous amount of papers submitted to all of the big ML conferences, and this trend shows no signs of reversing.  This has two effects. Firstly, it makes the conferences themselves more competitive. Since the rate of submissions is growing faster than the size of the conference, there is more and more competition for each spot every year. This means that many good-but-not-amazing contributions are being rejected every year, not because they are bad, but simply due to space constraints.
Secondly, it leads to an big burden on the reviewers. Senior reviewers are given tons of papers with too little time to read them all, and area chairs are forced to rope in less-experienced researchers to help as a result. I’m a first-year grad student, and I was asked to review 5 full papers for ICML! I did my best, but I don’t delude myself into thinking my evaluation was as accurate, or feedback as useful, as more experienced senior reviewers.&lt;/p&gt;

&lt;p&gt;These two factors result in a huge number of “noisy middle” papers, in the 40th-90th percentile range, all of which are similar in quality; a small subset of these papers are selected for acceptance, based on the whims of the randomly-assigned reviewers and meta-reviewers. From the outside, these decisions are seemingly completely random.&lt;/p&gt;

&lt;p&gt;Straightforward or incremental contributions often fall into this noisy middle. These papers are valuable contributions, but it is easier for reviewers to nitpick and question the novelty of these sorts of works, often dragging them down out of the top 10%. However, these sorts of projects are typically very safe, so they are a good choice for researchers who are still trying to secure a foothold in the research community. Thus, disadvantaged researchers - the ones the peer review system is supposed to serve best - are precisely the people most likely to find themselves in the noisy middle, with their acceptance determined by a coin flip! I think this was the point Matt was trying to make. Academic career paths incentivize disadvantaged researchers to pursue incremental work to get a foothold, and then even if they do a great job, they are sometimes, randomly, not rewarded with the main thing conferences should provide: certification.&lt;/p&gt;

&lt;p&gt;Here’s two suggestions for easily-implemented but impactful changes to the peer review process that could help mitigate the variance issues. I’d love to hear people’s thoughts on these.&lt;/p&gt;

&lt;h4 id=&#34;1-be-more-explicit-about-certification&#34;&gt;1. Be More Explicit About Certification&lt;/h4&gt;

&lt;p&gt;Right now, the lines are blurred between certification and dissemination. For example, many consider being selected for an oral presentation to be more prestigious than just a poster presentation, simply because fewer papers are selected as orals. But the oral presentation selection criteria is sometimes very unclear - is it for “better” work, or is it just for work that an area chair thought would make a fun presentation?&lt;/p&gt;

&lt;p&gt;A similar issue arises with conference acceptance decisions themselves. Conferences are fundamentally limited in the amount of papers they can accept, because the conference is physically limited by the size of the venue. If we agree that the role of a conference is to certify papers, isn’t it a bit weird that this certification is &lt;em&gt;relative&lt;/em&gt;? Surely each paper should be evaluated as to whether it is a worthwhile contribution to science, independently from what other papers happen to be submitted that year.&lt;/p&gt;

&lt;p&gt;So my first suggestion is this: &lt;em&gt;change from a relative metric to a standalone evaluation&lt;/em&gt;. Conferences should accept or reject each paper by some fixed criteria, regardless of how many papers get submitted that year. If there end up being too many papers to physically fit in the venue, select a subset of accepted papers, at random, to invite. &lt;em&gt;This mitigates one major source of randomness from the certification process: the quality of the other papers in any given submission pool.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;One downside of the above approach is that no individual author can guarantee themselves an invite by doing good research. To fix that, conferences might consider certifying at multiple levels, similar to a grading scheme. Some papers are accepted with an A (best papers), some papers are accepted with a B, C, D, and some papers are rejected with an F. The A’s, B’s, and C’s are prioritized when assigning conference invitations, with the remaining invites randomly distributed among the D’s. Oral presentations could be randomly selected from the B’s, etc. The goal of this system is to ensure that the certification is a “sufficient statistic” for the quality of a paper, as evaluated by the conference. If Paper 1 gets a B and gives an oral, and Paper 2 gets a B but no oral, that difference is explicitly completely random, so it’s associated with no prestige. (If any non-random decision is made at any point, prestige seems inevitable, so chairs will have to fight the urge to handpick their favorite B’s to give orals.)&lt;/p&gt;

&lt;h4 id=&#34;2-disincentivize-bad-submissions&#34;&gt;2. Disincentivize Bad Submissions&lt;/h4&gt;

&lt;p&gt;One reason that there are so many submissions to every conference is that, well, there is no reason not to submit. Authors only stand to gain from throwing their metaphorical hat into the ring: worst case, they get some feedback, and best case, they get a lucky acceptance. However, it’s a tragedy of the commons: when reviewers are forced to shoulder the burden of reviewing all these extra “might-as-well” manuscripts, review quality goes down across the board.&lt;/p&gt;

&lt;p&gt;Peer review is only as good as the peers doing the reviewing, and so I think that it is fundamentally impossible to have an effective peer review process with an absurdly high reviewer load. There have been various proposals for how to combat this: for example, a desk rejection process, where some papers are not even reviewed before being discarded. I want to put one more suggestion out there, though: &lt;em&gt;disincentivize low-quality submissions by publishing all submitted work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This means that if you submit to NeurIPS and they give you an F (rejection), it’s a matter of public record. The paper won’t be released, and you can resubmit that work elsewhere, but the failure will always live on. (Ideally we’ll develop community norms around academic integrity that mandate including a section on your CV to report your failures. But if not, we can at least make it easy for potential employers to find that information.)&lt;/p&gt;

&lt;p&gt;Why would this be beneficial? Well, it should be immediately obvious that this will directly disincentivize people from submitting half-done work. Each submission will have to be hyper-polished to the best it can possibly be before being submitted. It seems impossible that the number of papers polished to this level will be anywhere close to the number of submissions that we see at major conferences today. Those who choose to repeatedly submit poor-quality work anyways will have their CVs marred with a string of Fs, cancelling out any certification benefits they had hoped to achieve.&lt;/p&gt;

&lt;p&gt;Will this slow down the pace of publication? Likely, yes. But in the era of arXiv, that seems like a good thing. Put your pre-print on arXiv, solicit feedback, plant a flag or two - that’s what it’s there for. When BERT can take over the field of NLP after sitting on arXiv for two months, it’s clear that from a knowledge dissemination perspective, arXiv does a great job. So the only thing being slowed down here is the &lt;em&gt;certification&lt;/em&gt; process. And maybe that’s a good thing. When it comes to submitting to conferences for formal evaluation - submissions that are guaranteed to burn the valuable reviewing hours of your peers - maybe forcing people to take some extra time to perfect their submissions.&lt;/p&gt;

&lt;p&gt;There are some other positive externalities, too. For one thing, this sort of policy might revive the communities in smaller, “mid-tier” venues. In the stochastic-acceptance, silent-rejection paradigm we currently follow, there’s no reason for authors to submit to venues other than the top venues, unless they know for certain that their paper will never get in to the top venue. Due to the high stochasticity of the acceptance process, this means that pretty much all papers submitted to non-NeurIPS venues will be in the bottom 40% or so of typical NeurIPS submissions. That means that almost all of the good work (and hence, almost all of the attention and prestige) is concentrated in the top venues.&lt;/p&gt;

&lt;p&gt;In the public-rejection paradigm, mid-tier venues whose acceptance standards are less rigorous than the top venues once again have a place as certifiers. Rather than submitting a paper to NeurIPS that might be a D or F, you can submit a paper to another venue and get an A or B. This is a great option! Hopefully, as more and more good researchers start doing this, prestige will become more spread among venues, further reducing both the reviewer and organizational load.&lt;/p&gt;

&lt;p&gt;Another benefit of this approach is clear when you consider the relationship between the many authors on a typical paper. In the current system, it&amp;rsquo;s basically to the advantage of any senior author to get their name on as many papers as possible. There&amp;rsquo;s no downside at all: racking up the publications can only ever be beneficial.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; The public-rejection paradigm would hugely alter this dynamic. Public rejection introduces an element of risk to every author for every submission; a failure lands on everyone’s CVs, not just the first author’s. This will force senior authors to be much more invested in the quality of their submissions. People might even remove themselves from papers where they haven&amp;rsquo;t closely read the submission in order to confirm its quality. Closer involvement from PIs would be a nearly-universal positive, especially in big labs where students can sometimes be left to fend somewhat for themselves.&lt;/p&gt;

&lt;p&gt;If this idea seems radical to you, keep in mind: OpenReview has been publishing failures for years, without much fuss. Anyone can go right now and find out who got papers rejected from ICLR 2017. For some reason, though, nobody does this! My proposal is as much about community norms towards rejected papers as it is about actual data released by conferences. I think we should, as a community, all agree look harshly upon people who submit lots of bad papers, and make sure everyone &lt;em&gt;knows&lt;/em&gt; that they are being held to this standard going forwards. This seems to me the cleanest, fairest way to fix the exponential growth of conference submissions.&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I’d love to get feedback from the community on this, on both my framing of this issue and my suggested changes. Do people agree or disagree? What obvious issues have I failed to consider?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to Surya Bhupatiraju, Ryan Cotterell, Adi Renduchintala, and Isaac Buckman for feedback when writing this post.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:0&#34;&gt;I focus on conferences here, but this mostly applies to journals too.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:0&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:1&#34;&gt;There are other complaints about peer review, of course. For example, the “turnaround rate” of peer review is very slow: oftentimes, by the time a paper is presented at a conference, it has already been read and built upon by the majority of the community. However, these issues seem to be orthogonal to the core purpose of double-blind peer review, so I’m not going to discuss then further here. We already have a tool for fast turnaround: arXiv. Peer review serves a unique purpose, so if we change it, we should focus on changes that let it better serve that purpose.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;There’s some rare exceptions, of course, such as if the first author commits academic fraud or something egregious, in which case the senior author gets penalized too.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow: The Confusing Parts (2)</title>
      <link>https://jacobbuckman.com/post/tensorflow-the-confusing-parts-2/</link>
      <pubDate>Mon, 17 Sep 2018 00:30:19 -0400</pubDate>
      
      <guid>https://jacobbuckman.com/post/tensorflow-the-confusing-parts-2/</guid>
      <description>

&lt;p&gt;&lt;em&gt;This post is the second of a series; click &lt;a href=&#34;https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for the previous post, or &lt;a href=&#34;https://jacobbuckman.com/categories/tftcp/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for a list of all posts in this series.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;naming-and-scoping&#34;&gt;Naming and Scoping&lt;/h2&gt;

&lt;h3 id=&#34;naming-variables-and-tensors&#34;&gt;Naming Variables and Tensors&lt;/h3&gt;

&lt;p&gt;As we discussed in Part 1, every time you call &lt;code&gt;tf.get_variable()&lt;/code&gt;, you need to assign the variable a new, unique name. Actually, it goes deeper than that: every tensor in the graph gets a unique name too. The name can be accessed explicitly with the &lt;code&gt;.name&lt;/code&gt; property of tensors, operations, and variables. For the vast majority of cases, the name will be created automatically for you; for example, a constant node will have the name &lt;code&gt;Const&lt;/code&gt;, and as you create more of them, they will become &lt;code&gt;Const_1&lt;/code&gt;, &lt;code&gt;Const_2&lt;/code&gt;, etc.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:0&#34;&gt;&lt;a href=&#34;#fn:0&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; You can also explicitly set the name of a node via the &lt;code&gt;name=&lt;/code&gt; property, and the enumerative suffix will still be added automatically:&lt;/p&gt;

&lt;h6 id=&#34;code&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.constant(0.)
b = tf.constant(1.)
c = tf.constant(2., name=&amp;quot;cool_const&amp;quot;)
d = tf.constant(3., name=&amp;quot;cool_const&amp;quot;)
print a.name, b.name, c.name, d.name
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Const:0 Const_1:0 cool_const:0 cool_const_1:0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Explicitly naming nodes is nonessential, but can be very useful when debugging. Oftentimes, when your Tensorflow code crashes, the error trace will refer to a specific operation. If you have many operations of the same type, it can be tough to figure out which one is problematic. By explicitly naming each of your nodes, you can get much more informative error traces, and identify the issue more quickly.&lt;/p&gt;

&lt;h3 id=&#34;using-scopes&#34;&gt;Using Scopes&lt;/h3&gt;

&lt;p&gt;As your graph gets more complex, it becomes difficult to name everything by hand. Tensorflow provides the &lt;code&gt;tf.variable_scope&lt;/code&gt; object, which makes it easier to organize your graphs by subdividing them into smaller chunks. By simply wrapping a segment of your graph creation code in a &lt;code&gt;with tf.variable_scope(scope_name):&lt;/code&gt; statement, all nodes created will have their names automatically prefixed with the &lt;code&gt;scope_name&lt;/code&gt; string. Additionally, these scopes stack; creating a scope within another will simply chain the prefixes together, delimited by a forward-slash.&lt;/p&gt;

&lt;h6 id=&#34;code-1&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.constant(0.)
b = tf.constant(1.)
with tf.variable_scope(&amp;quot;first_scope&amp;quot;):
  c = a + b
  d = tf.constant(2., name=&amp;quot;cool_const&amp;quot;)
  coef1 = tf.get_variable(&amp;quot;coef&amp;quot;, [], initializer=tf.constant_initializer(2.))
  with tf.variable_scope(&amp;quot;second_scope&amp;quot;):
    e = coef1 * d
    coef2 = tf.get_variable(&amp;quot;coef&amp;quot;, [], initializer=tf.constant_initializer(3.))
    f = tf.constant(1.)
    g = coef2 * f
    
print a.name, b.name
print c.name, d.name
print e.name, f.name, g.name
print coef1.name
print coef2.name

&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-1&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Const:0 Const_1:0
first_scope/add:0 first_scope/cool_const:0
first_scope/second_scope/mul:0 first_scope/second_scope/Const:0 first_scope/second_scope/mul_1:0
first_scope/coef:0
first_scope/second_scope/coef:0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that we were able to create two variables with the same name - &lt;code&gt;coef&lt;/code&gt; - without any issues! This is because the scoping transformed the names into &lt;code&gt;first_scope/coef:0&lt;/code&gt; and &lt;code&gt;first_scope/second_scope/coef:0&lt;/code&gt;, which are distinct.&lt;/p&gt;

&lt;h2 id=&#34;saving-and-loading&#34;&gt;Saving and Loading&lt;/h2&gt;

&lt;p&gt;At its core, a trained neural network consists of two essential components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;weights&lt;/em&gt; of the network, which have been learned to optimize for some task&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;network graph&lt;/em&gt;, which specifies how to actually use the weights to get results&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tensorflow separates these two components, but it&amp;rsquo;s clear that they need to be very tightly paired.
Weights are useless without a graph structure describing how to use them, and a graph with random weights is no good either.
In fact, even something as small as swapping two weight matrices is likely to totally break your model.
This often leads to frustration among beginner Tensorflow users; using a pre-trained model as a component of a neural network is a great way to speed up training, but can break things in a myriad of ways.&lt;/p&gt;

&lt;h3 id=&#34;saving-a-model&#34;&gt;Saving A Model&lt;/h3&gt;

&lt;p&gt;When working with only a single model, Tensorflow&amp;rsquo;s built-in tools for saving and loading are straightforward to use: simply create a &lt;code&gt;tf.train.Saver()&lt;/code&gt;.
Similarly to the &lt;code&gt;tf.train.Optimizer&lt;/code&gt; family, a &lt;code&gt;tf.train.Saver&lt;/code&gt; is not itself a node, but instead a higher-level class that performs useful functions on top of pre-existing graphs.
And, as you may have anticipated, the &amp;lsquo;useful function&amp;rsquo; of a &lt;code&gt;tf.train.Saver&lt;/code&gt; is saving and loading the model.
Let&amp;rsquo;s see it in action!&lt;/p&gt;

&lt;h6 id=&#34;code-2&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
b = tf.get_variable(&#39;b&#39;, [])
init = tf.global_variables_initializer()

saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, &#39;./tftcp.model&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-2&#34;&gt;Output&lt;/h6&gt;

&lt;p&gt;Four new files:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;checkpoint
tftcp.model.data-00000-of-00001
tftcp.model.index
tftcp.model.meta
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s a lot of stuff to break down here.&lt;/p&gt;

&lt;p&gt;First of all: Why does it output &lt;em&gt;four&lt;/em&gt; files, when we only saved one model?
The information needed to recreate the model is divided among them.
If you want to copy or back up a model, make sure you bring all three of the files (the three prefixed by your filename).
Here&amp;rsquo;s a quick description of each:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tftcp.model.data-00000-of-00001&lt;/code&gt; contains the weights of your model (the first bullet point from above). It&amp;rsquo;s most likely the largest file here.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tftcp.model.meta&lt;/code&gt; is the network structure of your model (the second bullet point from above). It contains all the information needed to re-create your graph.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tftcp.model.index&lt;/code&gt; is an indexing structure linking the first two things. It says &amp;ldquo;where in the data file do I find the parameters corresponding to this node?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;checkpoint&lt;/code&gt; is not actually needed to reconstruct your model, but if you save multiple versions of your model throughout a training run, it keeps track of everything.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Secondly, why did I go through all the trouble of creating a &lt;code&gt;tf.Session&lt;/code&gt; and &lt;code&gt;tf.global_variables_initializer&lt;/code&gt; for this example?&lt;/p&gt;

&lt;p&gt;Well, if we&amp;rsquo;re going to save a model, we need to have something to save.
Recall that computations live in the graph, but values live in the session.
The &lt;code&gt;tf.train.Saver&lt;/code&gt; can access the structure of the network through a global pointer to the graph.
But when we go to save the &lt;em&gt;values of the variables&lt;/em&gt; (i.e. the weights of the network), we need to access a &lt;code&gt;tf.Session&lt;/code&gt; to see what those values are; that&amp;rsquo;s why &lt;code&gt;sess&lt;/code&gt; is passed in as the first argument of the &lt;code&gt;save&lt;/code&gt; function.
Additionally, attempting to save uninitialized variables will throw an error, because attempting to access the value of an uninitialized variable always throws an error.
So, we needed both a session and an initializer (or equivalent, e.g. &lt;code&gt;tf.assign&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&#34;loading-a-model&#34;&gt;Loading A Model&lt;/h3&gt;

&lt;p&gt;Now that we&amp;rsquo;ve saved our model, let&amp;rsquo;s load it back in.
The first step is to recreate the variables: we want variables with all the same names, shapes, and dtypes as we had when we saved it.
The second step is to create a &lt;code&gt;tf.train.Saver&lt;/code&gt; just as before, and call the &lt;code&gt;restore&lt;/code&gt; function.&lt;/p&gt;

&lt;h6 id=&#34;code-3&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
b = tf.get_variable(&#39;b&#39;, [])

saver = tf.train.Saver()
sess = tf.Session()
saver.restore(sess, &#39;./tftcp.model&#39;)
sess.run([a,b])
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-3&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[1.3106428, 0.6413864]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that we didn&amp;rsquo;t need to initialize &lt;code&gt;a&lt;/code&gt; or &lt;code&gt;b&lt;/code&gt; before running them!
This is because the &lt;code&gt;restore&lt;/code&gt; operation moves the values from our files into the session&amp;rsquo;s variables.
Since the session no longer contains any null-valued variables, initialization is no longer needed.
(This can backfire if we aren&amp;rsquo;t careful: running an init &lt;em&gt;after&lt;/em&gt; a restore will override the loaded values with randomly-initialized ones.)&lt;/p&gt;

&lt;h3 id=&#34;choosing-your-variables&#34;&gt;Choosing Your Variables&lt;/h3&gt;

&lt;p&gt;When a &lt;code&gt;tf.train.Saver&lt;/code&gt; is initialized, it looks at the current graph and gets the list of variables; this is permanently stored as the list of variables that that saver &amp;ldquo;cares about&amp;rdquo;.
We can inspect it with the &lt;code&gt;._var_list&lt;/code&gt; property:&lt;/p&gt;

&lt;h6 id=&#34;code-4&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
b = tf.get_variable(&#39;b&#39;, [])
saver = tf.train.Saver()
c = tf.get_variable(&#39;c&#39;, [])
print saver._var_list
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-4&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&amp;lt;tf.Variable &#39;a:0&#39; shape=() dtype=float32_ref&amp;gt;, &amp;lt;tf.Variable &#39;b:0&#39; shape=() dtype=float32_ref&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since &lt;code&gt;c&lt;/code&gt; wasn&amp;rsquo;t around at the time of our saver&amp;rsquo;s creation, it does not get to be a part of the fun.
So in general, make sure that you already have all your variables created before creating a saver.&lt;/p&gt;

&lt;p&gt;Of course, there are also some specific circumstances where you may actually want to only save a subset of your variables!
&lt;code&gt;tf.train.Saver&lt;/code&gt; lets you pass the &lt;code&gt;var_list&lt;/code&gt; when you create it to specify which subset of available variables you want it to keep track of.&lt;/p&gt;

&lt;h6 id=&#34;code-5&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
b = tf.get_variable(&#39;b&#39;, [])
c = tf.get_variable(&#39;c&#39;, [])
saver = tf.train.Saver(var_list=[a,b])
print saver._var_list
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-5&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&amp;lt;tf.Variable &#39;a:0&#39; shape=() dtype=float32_ref&amp;gt;, &amp;lt;tf.Variable &#39;b:0&#39; shape=() dtype=float32_ref&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;loading-modified-models&#34;&gt;Loading Modified Models&lt;/h3&gt;

&lt;p&gt;The examples above cover the &amp;lsquo;perfect sphere in frictionless vacuum&amp;rsquo; scenario of model-loading.
As long as you are saving and loading your own models, using your own code, without changing things in between, saving and loading is a breeze.
But in many cases, things are not so clean.
And in those cases, we need to get a little fancier.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at a couple of scenarios to illustrate the issues.
First, something that works without a problem.
What if we want to save a whole model, but we only want to load part of it?
(In the following code example, I run the two scripts in order.)&lt;/p&gt;

&lt;h6 id=&#34;code-6&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
b = tf.get_variable(&#39;b&#39;, [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, &#39;./tftcp.model&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.restore(sess, &#39;./tftcp.model&#39;)
sess.run(a)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-6&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1.1700551
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Good, easy enough!
And yet, a failure case emerges when we have the reverse scenario: we want to load one model as a component of a larger model.&lt;/p&gt;

&lt;h6 id=&#34;code-7&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, &#39;./tftcp.model&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
d = tf.get_variable(&#39;d&#39;, [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.restore(sess, &#39;./tftcp.model&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-7&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Key d not found in checkpoint
         [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We just wanted to load &lt;code&gt;a&lt;/code&gt;, while ignoring the new variable &lt;code&gt;d&lt;/code&gt;. And yet, we got an error, complaining that &lt;code&gt;d&lt;/code&gt; was not present in the checkpoint!&lt;/p&gt;

&lt;p&gt;A third scenario is where you want to load one model&amp;rsquo;s parameters into a &lt;em&gt;different&lt;/em&gt; model&amp;rsquo;s computation graph.
This throws an error too, for obvious reasons: Tensorflow cannot possibly know where to put all those parameters you just loaded.
Luckily, there&amp;rsquo;s a way to give it a hint.&lt;/p&gt;

&lt;p&gt;Remember &lt;code&gt;var_list&lt;/code&gt; from one section-header ago?
Well, it turns out to be a bit of a misnomer.
A better name might be &amp;ldquo;var_list_or_dictionary_mapping_names_to_vars&amp;rdquo;, but that&amp;rsquo;s a mouthful, so I can sort of see why they stuck with the first bit.&lt;/p&gt;

&lt;p&gt;Saving models is one of the key reasons that Tensorflow mandates globally-unique variable names.
In a saved-model-file, each saved variable&amp;rsquo;s name is associated with its shape and value.
Loading it into a new computational graph is as easy as mapping the original-names of the variables you want to load to variables in your current model.
Here&amp;rsquo;s an example:&lt;/p&gt;

&lt;h6 id=&#34;code-8&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, &#39;./tftcp.model&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
d = tf.get_variable(&#39;d&#39;, [])
init = tf.global_variables_initializer()
saver = tf.train.Saver(var_list={&#39;a&#39;: d})
sess = tf.Session()
sess.run(init)
saver.restore(sess, &#39;./tftcp.model&#39;)
sess.run(d)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-8&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;-0.9303965
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the key mechanism by which you can combine models that do not have the exact same computational graph.
For example, perhaps you got a pre-trained language model off of the internet, and want to re-use the word embeddings.
Or, perhaps you changed the parameterization of your model in between training runs, and you want this new version to pick up where the old one left off; you don&amp;rsquo;t want to have to re-train the whole thing from scratch.
In both of these cases, you would simply need to hand-make a dictionary mapping from the old variable names to the new variables.&lt;/p&gt;

&lt;p&gt;A word of caution: it&amp;rsquo;s very important to know &lt;em&gt;exactly&lt;/em&gt; how the parameters you are loading are meant to be used.
If possible, you should use the exact code the original authors used to build their model, to ensure that that component of your computational graph is identical to how it looked during training.
If you need to re-implement, keep in mind that basically any change, no matter how minor, is likely to severely damage the performance of your pre-trained net.
Always benchmark your reimplementation against the original!&lt;/p&gt;

&lt;h4 id=&#34;inspecting-models&#34;&gt;Inspecting Models&lt;/h4&gt;

&lt;p&gt;If the model you want to load came from the internet - or from yourself, &amp;gt;2 months ago - there&amp;rsquo;s a good chance you won&amp;rsquo;t &lt;em&gt;know&lt;/em&gt; how the original variables were named.
To inspect saved models, use &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/framework/checkpoint_utils.py&#34; target=&#34;_blank&#34;&gt;these tools&lt;/a&gt;, which come from the official Tensorflow repository.
For example:&lt;/p&gt;

&lt;h6 id=&#34;code-9&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.get_variable(&#39;a&#39;, [])
b = tf.get_variable(&#39;b&#39;, [10,20])
c = tf.get_variable(&#39;c&#39;, [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, &#39;./tftcp.model&#39;)
print tf.contrib.framework.list_variables(&#39;./tftcp.model&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-9&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[(&#39;a&#39;, []), (&#39;b&#39;, [10, 20]), (&#39;c&#39;, [])]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With a little effort and a lot of head-scratching, it&amp;rsquo;s usually possible to use these tools (in conjunction with the original codebase) to find the names of the variables you want.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Hopefully this post helped clear up the basics behind saving and loading Tensorflow models.
There are a few other advanced tricks, like automatic checkpointing and saving/restoring meta-graphs, that I may touch on in a future post; but in my experience, those use-cases are rare, especially for beginners.
As always, please let me know in the comments or via email if I got anything wrong, or there is anything important I missed.
Thanks for reading!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:0&#34;&gt;There will also be a suffix &lt;code&gt;:output_num&lt;/code&gt; added to the tensor names. For now, that&amp;rsquo;s always &lt;code&gt;:0&lt;/code&gt;, since we are only using operations with a single output. See &lt;a href=&#34;https://stackoverflow.com/questions/40925652/in-tensorflow-whats-the-meaning-of-0-in-a-variables-name&#34; target=&#34;_blank&#34;&gt;this StackOverflow question for more info&lt;/a&gt;. Thanks Su Tang for pointing this out!
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:0&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>OpenAI Five Takeaways</title>
      <link>https://jacobbuckman.com/post/openaifive-takeaways/</link>
      <pubDate>Mon, 06 Aug 2018 18:38:24 -0400</pubDate>
      
      <guid>https://jacobbuckman.com/post/openaifive-takeaways/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://blog.openai.com/openai-five-benchmark-results/&#34; target=&#34;_blank&#34;&gt;On August 5th, OpenAI successfully defeated top human players in a Dota 2 best-of-three series&lt;/a&gt;.
Their AI Dota agent, called OpenAI Five, was a deep neural network trained using reinforcement learning.
As a researcher studying deep reinforcement learning, as well as a long-time follower of competitive Dota 2, I found this face-off really interesting.
The eventual OAI5 victory was both impressive and well-earned - congrats to the team at OpenAI, and props to the humans for a hard-fought battle!
Of course, this result has provoked a lot of discussion; here&amp;rsquo;s my thoughts.&lt;/p&gt;

&lt;h3 id=&#34;how-did-openai-five-win&#34;&gt;How did OpenAI Five win?&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m sure this series will be analyzed by people with far deeper understanding of Dota than me, but in my opinion, OpenAI Five essentially won on the back of its teamfighting ability.
Good positioning and well-coordinated ability usage made it almost impossible to take 5v5 fights against the bot once it got started pushing.
During the laning phase, both teams were able to find pickoffs, and the human team actually pulled ahead in farm.
During the mid-game, whenever the bot team was not actively pushing, the humans were more efficient in getting resources from around the map.
Many times throughout the game, OAI5 did actions that were almost unequivocally bad, such as the pointless Smoke of Deceit usages we saw a couple of times.
But once OAI5 formed up a 5-man squad and started to push, the humans were steamrolled.&lt;/p&gt;

&lt;p&gt;This plan was followed to a t on both of the first two games, leading to decisive OAI5 victories.
During game three, it looked like the bots wanted to try the same thing again, but after their poor hero composition led to lost fights, they began simply doing anything they could to extend the game.
It certainly seems reasonable to me to assert that OAI5 was able to discover a near-unbeatable strategy, and learned to execute it perfectly.
In my opinion, these are the two main interesting outcomes of this result.
I&amp;rsquo;d like to describe why I find them surprising, and highlight some caveats.&lt;/p&gt;

&lt;h3 id=&#34;discovering-an-unbeatable-strategy&#34;&gt;Discovering an unbeatable strategy&lt;/h3&gt;

&lt;p&gt;Dota 2 has massive state and action spaces - according to &lt;a href=&#34;https://blog.openai.com/openai-five/&#34; target=&#34;_blank&#34;&gt;an earlier blog post&lt;/a&gt;, the dimensionalities of the state and action vectors are 20,000 and 1,000, respectively.
Right from the start, there is a combinatorial explosion in observed state stemming from team compositions; these quickly grow even more differentiated based on skill and item builds.
Further complicating things, the action space is a mix of discrete choices (i.e. whether to move or use a skill, what skill to use) and continuous parameters (i.e. where to move).
This results in an enormous set of states that need to be explored and remembered, and a potentially huge amount of promising actions in each one.&lt;/p&gt;

&lt;p&gt;When I first heard about OpenAI tackling Dota 2, I assumed that exploration would be one of the major challenges.
Surely standard noise-based or epsilon-greedy approaches would fall short - in this insanely large state space, what are the chances that it will stumble upon good gameplay?&lt;/p&gt;

&lt;p&gt;But in the absence of an explicit comment in any press release about exploration, I assume that OpenAI&amp;rsquo;s exploration techniques were, in fact, totally standard.
It seems that through nothing but the reward engineering efforts of the members of the development team, OAI5 was able to get a dense enough reward signal that it discovered a near-unbeatable strategy.
This is pretty surprising so me: I&amp;rsquo;m a big believer in the end-to-end RL dream (i.e. training exclusively from the win/loss reward signal under the assumption that any human intuition we add to the training process will &amp;ldquo;dilute&amp;rdquo; the purity of the solution discovered with our pesky suboptimalities).
My thinking was that in any real-world problem, effective reward engineering would be so difficult as to be impossible.
&lt;a href=&#34;https://www.youtube.com/watch?v=tlOIHko8ySg&#34; target=&#34;_blank&#34;&gt;There are many examples of reward-engineering-gone-wrong in various domains&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Those in favor of reward engineering argue that it&amp;rsquo;s possible to avoid these issues with careful engineering.
It&amp;rsquo;s true that an AI trained in an environment that rewards last-hitting is unlikely to ever learn a revolutionary new strategy that ignores last-hits entirely.
But the chance that such a strategy exists seems quite low, and it&amp;rsquo;s clearly possible to learn a strategy that &lt;em&gt;both&lt;/em&gt; secures last-hits and wins games.
So, why not do it? These &amp;ldquo;reward breadcrumbs&amp;rdquo; will guide us towards certain areas of the solution space and away from others.
But as long as we are careful to ensure that the getting the engineered reward is at least compatible with overall success on the task, it may be an indispensible part of solving real-world RL problems.&lt;/p&gt;

&lt;p&gt;Dota 2 is the first example I&amp;rsquo;ve seen of this in practice; a challenging real-world (ish) task with a seemingly impossible exploration issue, that was overcome by clever reward engineering.
Maybe it&amp;rsquo;s true that another agent, trained in the exact same way but with just the win/loss reward, could eventually learn a better strategy than OpenAI Five.
But until someone manages to make one, I don&amp;rsquo;t see much reason to believe that.
And it seems likely that even if such a system existed, it would almost certainly take a lot longer to train.
So seeing this result has begun to win me over in favor of reward engineering on real-world tasks.&lt;/p&gt;

&lt;h4 id=&#34;caveats&#34;&gt;Caveats&lt;/h4&gt;

&lt;p&gt;Of course, this would likely have still been inadequate without scale.
In the millions upon millions of games played by the system, even naive exploration is able to cover an enormous amount of possible game-states.
And as massive as the current state and action space are, they are tiny compared to the eventual number of states once the remainder of the features are added.
The remaining ~100 heros will increase the state space exponentially, and adding in the ability to control multiple units (via illusions, summons, etc.) will increase the action space exponentially as well: in addition to choosing an action, the agent must choose between the $2^n$ potential subsets of controllable units at its disposal.
It remains to be seen whether OpenAI&amp;rsquo;s current exploration strategies will be able to keep up.&lt;/p&gt;

&lt;p&gt;Additionally, the very existence of an &amp;ldquo;unbeatable strategy&amp;rdquo; undermines some of the key reasons that Dota 2 was selected as an interesting platform for research.
For example, people often cite strategic decision making, long-term strategy, and complex opponent modeling and counter-play as fundamental to successful Dota.
But if an unbeatable strategy exists, that goes out the window.
Simply executing on a known strategy doesn&amp;rsquo;t require any higher-level reasoning or planning, and certainly doesn&amp;rsquo;t require much opponent modeling.
Framed in this way, I see these Dota victories as being in the same category as &lt;a href=&#34;https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/&#34; target=&#34;_blank&#34;&gt;learning to walk in a complex, noisy environment&lt;/a&gt;.
&amp;ldquo;Just do your thing, Dota bot, and do your best to correct for whatever those silly human opponents throw at you.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s entirely possible that OAI5 &lt;em&gt;would&lt;/em&gt; learn these higher-level behaviors, if trained to play a more balanced game.
Dota 2 is balanced around all 115 heroes and all items, but the version played by OAI5 uses a pool of only 18, and so any concept of balance goes out the window.
As Blitz said during the post-game panel discussion: the limited set of available heroes forced the human players to &amp;ldquo;play the bot&amp;rsquo;s game.&amp;rdquo;
Heroes with strong counter-push and teamfight like Venomancer, Enigma, and Naga Siren would have directly countered the strategy played by OAI5, and &amp;ldquo;backdoor&amp;rdquo; heroes like Furion and Lycan would have allowed the humans to play around the bots and gain advantage in many lanes at once.
Once these heroes are added to the pool, perhaps we will see a totally different set of strategies.&lt;/p&gt;

&lt;p&gt;(Also, Icefrog does his best, but there is no guarantee that even the full game of Dota 2 is balanced!
The game is constantly being tweaked to make more and more strategies viable.
Alliance&amp;rsquo;s nearly-undefeated run at TI3 on the back of a pushing strategy - somewhat similar to OAI5&amp;rsquo;s, in fact - led to a swift nerf of the heroes and items responsible.
And of course, who can forget the &amp;ldquo;Ho Ho Ha Ha&amp;rdquo; patch of 6.83, where picking Sniper or Troll Warlord would basically guarantee your team a victory.
So if a future OAI5, trained on the full game, is able to come up with a single, dominant strategy, expect that strategy to be patched into uselessness soon afterwards.)&lt;/p&gt;

&lt;h3 id=&#34;executing-perfectly-against-an-out-of-domain-opponent&#34;&gt;Executing perfectly against an out-of-domain opponent&lt;/h3&gt;

&lt;p&gt;In spite of some good-natured ribbing from the casters, I thought that OAI5 did astonishingly well in dealing with situations that is was not exposed to in training.
The concept of &amp;ldquo;pulling creeps&amp;rdquo; was not something that it had ever learned to do, and therefore also not something that it had learned to deal with.
But the bot handled the sudden lack of creep wave more-or-less as a surprised human would.
In the second game of the series, it was even able to capitalize on a pull that it had vision of and pick off a hero.&lt;/p&gt;

&lt;h4 id=&#34;caveats-1&#34;&gt;Caveats&lt;/h4&gt;

&lt;p&gt;Since this was only a three-game series, and the humans played mostly classic non-cheesy Dota 2, it&amp;rsquo;s hard to get a sense of just how stable the AI is when encountering unfamiliar situations.
But from the few games we&amp;rsquo;ve seen, I think it looks much less exploitable than its 1v1 counterpart.
The flipside of having to explore such a vast state space is that it seemingly becomes a lot harder to force the bot into a state that is &lt;em&gt;truly&lt;/em&gt; unexpected.
Even if some minor things are &amp;ldquo;off&amp;rdquo;, there are enough familiar elements that OAI5 is able to stick to its plan.
Whether this is a product of domain randomization and self-play, or simply a reflection of the increased importance of static objectives like towers in 5v5, I&amp;rsquo;m not certain.
(Or whether it is even a real effect - if OpenAI sets up a public free-for-all like they did last year, it might prove fragile after all!)&lt;/p&gt;

&lt;h3 id=&#34;on-cooperation&#34;&gt;On cooperation&lt;/h3&gt;

&lt;p&gt;The teamfight coordination displayed by OpenAI is absolutely incredible by human standards.
In humans, this level of teamfight requires months of practice and an intense focus on cooperation; it is reserved for the highest-level professional teams.
And yet, I don&amp;rsquo;t think that the various heroes controlled by the bot displayed anything resembling human cooperation.
When we say that two agents to are &amp;ldquo;cooperating&amp;rdquo;, I think an essential part of that definition involves reasoning under uncertainty.&lt;/p&gt;

&lt;p&gt;In humans playing Dota 2, I see two major sources of uncertainty: policy uncertainty and information uncertainty.&lt;/p&gt;

&lt;p&gt;To illustrate what I mean by policy uncertainty, consider what it would be like to be placed on a team with a group of random Dota players.
You don&amp;rsquo;t know exactly who the other players are, what they will do, how they will react.
At first, you must behave cautiously; you have some assumptions about what they will do, but you need to make sure you always have a backup plan in case they deviate.
As you play with them more, and get to know them better, you get a better sense of their actions, and they of yours.
Eventually, you can very reliably predict their actions, and trust them to have your back when needed.
But at the end of the day, there&amp;rsquo;s always still some uncertainty there.&lt;/p&gt;

&lt;p&gt;Information uncertainty arises when two players have a different subset of the information on the screen visible to them at any given time.
Though all human players can scroll their screen around to see everything visible to all team members, in practice, there is too much going on at any given time for a single player to absorb it all.
Therefore, even at the same moment of the same game, different players are likely to have taken different subsets of the available information as input.
Since it&amp;rsquo;s impossible to know exactly what each of your teammates was looking at, and they might take various actions depending on what they saw, this is a second major source of uncertainty.&lt;/p&gt;

&lt;p&gt;OAI5 is, by design, able to skirt these issues.
The network controlling each hero can take in the full state of the map at every tick, meaning that all heroes have access to identical information.
And since all of the millions of games played have had the same &amp;ldquo;player&amp;rdquo; on each hero, there is essentially no policy uncertainty - each hero can perfectly predict the actions of each other.
With neither policy nor information differences, OAI5 is able to display &amp;ldquo;perfect cooperation&amp;rdquo;, essentially controlling all five heroes as though they were a single entity.&lt;/p&gt;

&lt;p&gt;This is excellent for Dota bots, but I think it&amp;rsquo;s disingenuous to use the word &amp;ldquo;cooperation&amp;rdquo; to describe it, because it doesn&amp;rsquo;t generalize well to other settings where cooperation is important.
For one thing, I&amp;rsquo;d wager that OAI5 would be extremely crippled if any one of its heroes were replaced by a human player, of &lt;em&gt;any&lt;/em&gt; skill level.
Even if the 5th player is a top pro, playing excellently, he or she would still be playing &lt;em&gt;differently&lt;/em&gt; than expected, and the bot would be unable to cooperate as a result.
(I&amp;rsquo;d love to see OpenAI try this and let me know what the result is, though!)&lt;/p&gt;

&lt;p&gt;I think that this &amp;ldquo;perfect cooperation&amp;rdquo; is the Dota equivalent of &amp;ldquo;aimbotting&amp;rdquo; in FPS games - there&amp;rsquo;s a fundamental assumption about the limitations of human beings built into the game, and all top humans reach this physical limit, which means they are forced to compete with one another by using high-level reasioning and strategy.
By sidestepping that limitation, a bot is able to easily win without any of the interesting stuff.
I would be much more convinced that the bots are using high-level strategy and long-term planning if they were handicapped in a way that brought them back down to the level of humans in this regard.&lt;/p&gt;

&lt;p&gt;To incorporate policy uncertainty, I&amp;rsquo;d like to see OpenAI train an ensemble of many OAI5 agents.
In any given 5v5 game, 10 agents would be randomly sampled and assigned to the ten heroes; each agent only learns from games that it participates in.
This would force the agents to have some uncertainty about how their teammates will behave, since they get different teammates on different games.
And since the ensemble is just a bunch of copies of the agent that they&amp;rsquo;ve already made, it should be somewhat straightforward for OpenAI to set up.
(Of course, the implementation difficulty depends a bit on the architecture OpenAI used; if there is a lot of parameter sharing between agents, this will be hard.)&lt;/p&gt;

&lt;p&gt;Incorporating information uncertainty is less of an issue, in my opinion; the bot&amp;rsquo;s ability to take in all available information on the map seems like more of a &amp;ldquo;fair&amp;rdquo; advantage.
It could maybe be incorporated by randomly masking out certain segments of the input at every frame, perhaps dropping out inputs further from the agents at a higher rate.&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Well, that ended up being a bit more of a brain dump than I intended.
In summary, I think that this is a super impressive accomplishment, and says a lot about how good policy search, domain randomization, and reward shaping are at finding solutions.
However, I&amp;rsquo;m hesitant to draw any conclusions about whether the solution found involves much higher-level reasoning or long-term planning.
Thanks for reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sample-efficient Reinforcement Learning with Stochastic Ensemble Value Expansion</title>
      <link>https://jacobbuckman.com/publication/steve/</link>
      <pubDate>Mon, 06 Aug 2018 02:00:24 -0400</pubDate>
      
      <guid>https://jacobbuckman.com/publication/steve/</guid>
      <description></description>
    </item>
    
    <item>
      <title>More on Graph Inspection</title>
      <link>https://jacobbuckman.com/post/graph-inspection/</link>
      <pubDate>Sun, 05 Aug 2018 01:39:42 -0400</pubDate>
      
      <guid>https://jacobbuckman.com/post/graph-inspection/</guid>
      <description>

&lt;p&gt;In &lt;a href=&#34;https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/&#34; target=&#34;_blank&#34;&gt;Tensorflow: The Confusing Parts (1)&lt;/a&gt;, I described the abstractions underlying Tensorflow at a high level in an intuitive manner. In this follow-up post, I dig more deeply, and examine how these abstractions are actually implemented. Understanding these implementation details isn&amp;rsquo;t necessarily essential to writing and using Tensorflow, but it allows us to inspect and debug computational graphs.&lt;/p&gt;

&lt;h2 id=&#34;inspecting-graphs&#34;&gt;Inspecting Graphs&lt;/h2&gt;

&lt;p&gt;The computational graph is not just a nebulous, immaterial abstraction; it is a computational object that exists, and can be inspected. Complicated graphs are difficult to debug if we are representing them entirely in our heads, but inspecting and debugging the actual graph object makes thigs much easier.&lt;/p&gt;

&lt;p&gt;To access the graph object, use &lt;code&gt;tf.get_default_graph()&lt;/code&gt;, which returns a pointer to the global default graph object:&lt;/p&gt;

&lt;h6 id=&#34;code&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
g = tf.get_default_graph()
print g
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;lt;tensorflow.python.framework.ops.Graph object at 0x1144ffd90&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This object has the potential to tell us everything we need to know about the computational graph we have constructed. But only if we know how to use it! First, let&amp;rsquo;s take a step back and dive a bit deeper into something I glossed over in the first part: the difference between edges and nodes.&lt;/p&gt;

&lt;p&gt;The mathematical definition of a graph includes both edges and nodes. A Tensorflow graph is no exception: it has &lt;code&gt;tf.Operation&lt;/code&gt; objects (nodes) and &lt;code&gt;tf.Tensor&lt;/code&gt; objects (edges). An operation results in a single tensor (edge) as an output, so it&amp;rsquo;s fine to conflate the two in most cases; that&amp;rsquo;s what I did in TFTCP1. But in terms of the actual Python objects that make up the graph, they are programmatically distinct.&lt;/p&gt;

&lt;p&gt;When we create a new node, there are actually three things happening under the hood:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We gather up all the &lt;code&gt;tf.Tensor&lt;/code&gt; objects corresponding to the incoming edges for our new node&lt;/li&gt;
&lt;li&gt;We create a new node, which is a &lt;code&gt;tf.Operation&lt;/code&gt; object&lt;/li&gt;
&lt;li&gt;We create one or more new outgoing edges, which are &lt;code&gt;tf.Tensor&lt;/code&gt; objects, and return pointers to them&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are three primary ways that we can inspect the graph to understand how these pieces fit together:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;List All Nodes:&lt;/strong&gt; &lt;code&gt;tf.Graph.get_operations()&lt;/code&gt; returns all operations in the graph&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inspecting Nodes:&lt;/strong&gt; &lt;code&gt;tf.Operation.inputs&lt;/code&gt; and &lt;code&gt;tf.Operation.outputs&lt;/code&gt; each return a list of &lt;code&gt;tf.Tensor&lt;/code&gt; objects, which correspond to the incoming edges and outgoing edges, respectively&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inspecting Edges:&lt;/strong&gt; &lt;code&gt;tf.Tensor.op&lt;/code&gt; returns a single &lt;code&gt;tf.Operation&lt;/code&gt; for which this tensor is the output, and &lt;code&gt;tf.Tensor.consumers()&lt;/code&gt; returns a list of all &lt;code&gt;tf.Operations&lt;/code&gt; for which this tensor is used as an input.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here&amp;rsquo;s an example of these in action:&lt;/p&gt;

&lt;h6 id=&#34;code-1&#34;&gt;Code&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.constant(2, name=&#39;a&#39;)
b = tf.constant(3, name=&#39;b&#39;)
c = a + b

print &amp;quot;Our tf.Tensor objects:&amp;quot;
print a
print b
print c
print

a_op = a.op
b_op = b.op
c_op = c.op

print &amp;quot;Our tf.Operation objects, printed in compressed form:&amp;quot;
print a_op.__repr__()
print b_op.__repr__()
print c_op.__repr__()
print

print &amp;quot;The default behavior of printing a tf.Operation object is to pretty-print:&amp;quot;
print c_op

print &amp;quot;Inspect consumers for each tensor:&amp;quot;
print a.consumers()
print b.consumers()
print c.consumers()
print

print &amp;quot;Inspect input tensors for each op:&amp;quot;
# it&#39;s in a weird format, tensorflow.python.framework.ops._InputList, so we need to convert to list() to inspect
print list(a_op.inputs)
print list(b_op.inputs)
print list(c_op.inputs)
print

print &amp;quot;Inspect input tensors for each op:&amp;quot;
print a_op.outputs
print b_op.outputs
print c_op.outputs
print

print &amp;quot;The list of all nodes (tf.Operations) in the graph:&amp;quot;
g = tf.get_default_graph()
ops_list = g.get_operations()
print ops_list
print

print &amp;quot;The list of all edges (tf.Tensors) in the graph, by way of list comprehension:&amp;quot;
tensors_list = [tensor for op in ops_list for tensor in op.outputs]
print tensors_list
print

print &amp;quot;Note that these are the same pointers we can find by referring to our various graph elements directly:&amp;quot;
print ops_list[0] == a_op, tensors_list[0] == a
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-1&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Our tf.Tensor objects:
Tensor(&amp;quot;a:0&amp;quot;, shape=(), dtype=int32)
Tensor(&amp;quot;b:0&amp;quot;, shape=(), dtype=int32)
Tensor(&amp;quot;add:0&amp;quot;, shape=(), dtype=int32)

Our tf.Operation objects, printed in compressed form:
&amp;lt;tf.Operation &#39;a&#39; type=Const&amp;gt;
&amp;lt;tf.Operation &#39;b&#39; type=Const&amp;gt;
&amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;

The default behavior of printing a tf.Operation object is to pretty-print:
name: &amp;quot;add&amp;quot;
op: &amp;quot;Add&amp;quot;
input: &amp;quot;a&amp;quot;
input: &amp;quot;b&amp;quot;
attr {
  key: &amp;quot;T&amp;quot;
  value {
    type: DT_INT32
  }
}

Inspect consumers for each tensor:
[&amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;]
[&amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;]
[]

Inspect input tensors for each op:
[]
[]
[&amp;lt;tf.Tensor &#39;a:0&#39; shape=() dtype=int32&amp;gt;, &amp;lt;tf.Tensor &#39;b:0&#39; shape=() dtype=int32&amp;gt;]

Inspect input tensors for each op:
[&amp;lt;tf.Tensor &#39;a:0&#39; shape=() dtype=int32&amp;gt;]
[&amp;lt;tf.Tensor &#39;b:0&#39; shape=() dtype=int32&amp;gt;]
[&amp;lt;tf.Tensor &#39;add:0&#39; shape=() dtype=int32&amp;gt;]

The list of all nodes (tf.Operations) in the graph:
[&amp;lt;tf.Operation &#39;a&#39; type=Const&amp;gt;, &amp;lt;tf.Operation &#39;b&#39; type=Const&amp;gt;, &amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;]

The list of all edges (tf.Tensors) in the graph, by way of list comprehension:
[&amp;lt;tf.Tensor &#39;a:0&#39; shape=() dtype=int32&amp;gt;, &amp;lt;tf.Tensor &#39;b:0&#39; shape=() dtype=int32&amp;gt;, &amp;lt;tf.Tensor &#39;add:0&#39; shape=() dtype=int32&amp;gt;]

Note that these are the same pointers we can find by referring to our various graph elements directly:
True True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a couple of funky things that we have to do to make everything nice to look at, but once you get used to them, inspecting the graph becomes second nature.&lt;/p&gt;

&lt;p&gt;Of course, no discussion of graphs would be complete without taking a look at &lt;code&gt;tf.Variable&lt;/code&gt; objects too:&lt;/p&gt;

&lt;h6 id=&#34;code-2&#34;&gt;Code&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.constant(2, name=&#39;a&#39;)
b = tf.get_variable(&#39;b&#39;, [], dtype=tf.int32)
c = a + b

g = tf.get_default_graph()
ops_list = g.get_operations()
print

print &amp;quot;tf.Variable objects are really a bundle of four operations (and their corresponding tensors):&amp;quot;
print b
print ops_list
print

print &amp;quot;Two of these are accessed via their tf.Operations,&amp;quot;,
print &amp;quot;the core&amp;quot;, b.op.__repr__(), &amp;quot;and the initializer&amp;quot;, b.initializer.__repr__()
print &amp;quot;The other two are accessed via their tf.Tensors,&amp;quot;,
print &amp;quot;the initial-value&amp;quot;, b.initial_value, &amp;quot;and the current-value&amp;quot;, b.value()
print

print &amp;quot;A tf.Variable core-op takes no inputs, and outputs a tensor of type *_ref:&amp;quot;
print b.op.__repr__()
print list(b.op.inputs), b.op.outputs
print

print &amp;quot;A tf.Variable current-value is the output of a \&amp;quot;/read\&amp;quot; operation, which converts from *_ref to a tensor with a concrete data-type.&amp;quot;
print &amp;quot;Other ops use the concrete node as their input:&amp;quot;
print b.value()
print b.value().op.__repr__()
print list(c.op.inputs)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-2&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.Variable objects are really a bundle of four operations (and their corresponding tensors):
&amp;lt;tf.Variable &#39;b:0&#39; shape=() dtype=int32_ref&amp;gt;
[&amp;lt;tf.Operation &#39;a&#39; type=Const&amp;gt;, &amp;lt;tf.Operation &#39;b/Initializer/zeros&#39; type=Const&amp;gt;, &amp;lt;tf.Operation &#39;b&#39; type=VariableV2&amp;gt;, &amp;lt;tf.Operation &#39;b/Assign&#39; type=Assign&amp;gt;, &amp;lt;tf.Operation &#39;b/read&#39; type=Identity&amp;gt;, &amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;]

Two of these are accessed via their tf.Operations, the core &amp;lt;tf.Operation &#39;b&#39; type=VariableV2&amp;gt; and the initializer &amp;lt;tf.Operation &#39;b/Assign&#39; type=Assign&amp;gt;
The other two are accessed via their tf.Tensors, the initial-value Tensor(&amp;quot;b/Initializer/zeros:0&amp;quot;, shape=(), dtype=int32) and the current-value Tensor(&amp;quot;b/read:0&amp;quot;, shape=(), dtype=int32)

A tf.Variable core-op takes no inputs, and outputs a tensor of type *_ref:
&amp;lt;tf.Operation &#39;b&#39; type=VariableV2&amp;gt;
[] [&amp;lt;tf.Tensor &#39;b:0&#39; shape=() dtype=int32_ref&amp;gt;]

A tf.Variable current-value is the output of a &amp;quot;/read&amp;quot; operation, which converts from *_ref to a tensor with a concrete data-type.
Other ops use the concrete node as their input:
Tensor(&amp;quot;b/read:0&amp;quot;, shape=(), dtype=int32)
&amp;lt;tf.Operation &#39;b/read&#39; type=Identity&amp;gt;
[&amp;lt;tf.Tensor &#39;a:0&#39; shape=() dtype=int32&amp;gt;, &amp;lt;tf.Tensor &#39;b/read:0&#39; shape=() dtype=int32&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So a &lt;code&gt;tf.Variable&lt;/code&gt; adds (at least) four ops, but most of the details can be happily abstracted away by the &lt;code&gt;tf.Variable&lt;/code&gt; interface. In general, you can just assume that a &lt;code&gt;tf.Variable&lt;/code&gt; will be the thing you want it to be in any given circumstance. For example, if you want to assign a value to a variable, it will resolve to the core-op; if you want to use the variable in a computation, it will resolve to the current-value-op; etc.&lt;/p&gt;

&lt;p&gt;Take some time to play around with inspecting simple Tensorflow graphs in a Colab or interpreter - it will pay off in time saved debugging later!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow: The Confusing Parts (1)</title>
      <link>https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/</link>
      <pubDate>Mon, 25 Jun 2018 14:53:44 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/</guid>
      <description>

&lt;p&gt;&lt;em&gt;This post is the first of a series; click &lt;a href=&#34;https://jacobbuckman.com/post/tensorflow-the-confusing-parts-2/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for the next post, or &lt;a href=&#34;https://jacobbuckman.com/categories/tftcp/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for a list of all posts in this series.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;#understanding-tensorflow&#34;&gt;Click here to skip the intro and dive right in!&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;h3 id=&#34;what-is-this-who-are-you&#34;&gt;What is this? Who are you?&lt;/h3&gt;

&lt;p&gt;I’m Jacob, a &lt;a href=&#34;https://ai.google/research/join-us/ai-residency/&#34; target=&#34;_blank&#34;&gt;Google AI Resident&lt;/a&gt;. When I started the residency program in the summer of 2017, I had a lot of experience programming, and a good understanding of machine learning, but I had never used Tensorflow before. I figured that given my background I’d be able to pick it up quickly.  To my surprise, the learning curve was fairly steep, and even months into the residency, I would occasionally find myself confused about how to turn ideas into Tensorflow code. I’m writing this blog post as a message-in-a-bottle to my former self: it’s the introduction that I wish I had been given before starting on my journey. Hopefully, it will also be a helpful resource for others.&lt;/p&gt;

&lt;h3 id=&#34;what-was-missing&#34;&gt;What was missing?&lt;/h3&gt;

&lt;p&gt;In the three years since its release, &lt;a href=&#34;https://github.com/thedataincubator/data-science-blogs/blob/master/output/DL_libraries_final_Rankings.csv&#34; target=&#34;_blank&#34;&gt;Tensorflow has cemented itself as a cornerstone of the deep learning ecosystem&lt;/a&gt;. However, it can be non-intuitive for beginners, especially compared to define-by-run&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:0&#34;&gt;&lt;a href=&#34;#fn:0&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; neural network libraries like &lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34;&gt;PyTorch&lt;/a&gt; or &lt;a href=&#34;http://dynet.io&#34; target=&#34;_blank&#34;&gt;DyNet&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Many introductory Tensorflow tutorials exist, for doing everything from &lt;a href=&#34;https://www.tensorflow.org/tutorials/wide&#34; target=&#34;_blank&#34;&gt;linear regression&lt;/a&gt;, to &lt;a href=&#34;https://www.tensorflow.org/tutorials/layers&#34; target=&#34;_blank&#34;&gt;classifying MNIST&lt;/a&gt;, to &lt;a href=&#34;https://www.tensorflow.org/tutorials/seq2seq&#34; target=&#34;_blank&#34;&gt;machine translation&lt;/a&gt;. These concrete, practical guides are great resources for getting Tensorflow projects up and running, and can serve as jumping-off points for similar projects. But for the people who are working on applications for which a good tutorial does not exist, or who want to do something totally off the beaten path (as is common in research), Tensorflow can definitely feel frustrating at first.&lt;/p&gt;

&lt;p&gt;This post is my attempt to fill this gap. Rather than focusing on a specific task, I take a more general approach, and explain the fundamental abstractions underpinning Tensorflow. With a good grasp of these concepts, deep learning with Tensorflow becomes intuitive and straightforward.&lt;/p&gt;

&lt;h3 id=&#34;target-audience&#34;&gt;Target Audience&lt;/h3&gt;

&lt;p&gt;This tutorial is intended for people who already have some experience with both programming and machine learning, and want to pick up Tensorflow. For example: a computer science student who wants to use Tensorflow in the final project of her ML class; a software engineer who has just been assigned to a project that involves deep learning; or a bewildered new Google AI Resident (shout-out to past Jacob). If you’d like a refresher on the basics, &lt;a href=&#34;https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; &lt;a href=&#34;http://colah.github.io/&#34; target=&#34;_blank&#34;&gt;are&lt;/a&gt; &lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning--ud120&#34; target=&#34;_blank&#34;&gt;some&lt;/a&gt; &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34; target=&#34;_blank&#34;&gt;resources&lt;/a&gt;. Otherwise: let’s get started!&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;understanding-tensorflow&#34;&gt;Understanding Tensorflow&lt;/h1&gt;

&lt;h3 id=&#34;tensorflow-is-not-a-normal-python-library&#34;&gt;Tensorflow Is Not A Normal Python Library&lt;/h3&gt;

&lt;p&gt;Most Python libraries are written to be natural extensions of Python. When you import a library, what you get is a set of variables, functions, and classes, that augment and complement your “toolbox” of code. When using them, you have a certain set of expectations about how they behave. In my opinion, when it comes to Tensorflow, you should throw all that away. It’s fundamentally the wrong way to think about what Tensorflow is and how it interacts with the rest of your code.&lt;/p&gt;

&lt;p&gt;A metaphor for the relationship between Python and Tensorflow is the relationship between Javascript and HTML. Javascript is a fully-featured programming language that can do all sorts of wonderful things. HTML is a framework for representing a certain type of useful computational abstraction (in this case, content that can be rendered by a web browser). The role of Javascript in an interactive webpage is to assemble the HTML object that the browser sees, and then interact with it when necessary by updating it to new HTML.&lt;/p&gt;

&lt;p&gt;Similarly to HTML, Tensorflow is a framework for representing a certain type of computational abstraction (known as “computation graphs”). When we manipulate Tensorflow with Python, the first thing we do with our Python code is assemble the computation graph. Once that is done, the second thing we do is to interact with it (using Tensorflow’s “sessions”). But it’s important to keep in mind that the computation graph does not live inside of your variables; it lives in the global namespace. As Shakespeare once said: “All the RAM’s a stage, and all the variables are merely pointers.”&lt;/p&gt;

&lt;h3 id=&#34;first-key-abstraction-the-computation-graph&#34;&gt;First Key Abstraction: The Computation Graph&lt;/h3&gt;

&lt;p&gt;In browsing the Tensorflow documentation, you’ve probably found oblique references to “graphs” and “nodes”. If you’re a particularly savvy browser, you may have even discovered &lt;a href=&#34;https://www.tensorflow.org/programmers_guide/graphs&#34; target=&#34;_blank&#34;&gt;this page&lt;/a&gt;, which covers the content I’m about to explain in a much more accurate and technical fashion. This section is a high-level walkthrough that captures the important intuition, while sacrificing some technical details.&lt;/p&gt;

&lt;p&gt;So: what is a computation graph? Essentially, it’s a global data structure: a directed graph that captures instructions about how to calculate things.&lt;/p&gt;

&lt;p&gt;Let’s walk through an example of how to build one. In the following figures, the top half is the code we ran and its output, and the bottom half is the resulting computation graph.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig0.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Predictably, just importing Tensorflow does not give us an interesting computation graph. Just a lonely, empty global variable. But what about when we call a Tensorflow operation?&lt;/p&gt;

&lt;h6 id=&#34;code&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
print two_node
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Tensor(&amp;quot;Const:0&amp;quot;, shape=(), dtype=int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-1&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig1.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Would you look at that! We got ourselves a node. It contains the constant 2. Shocking, I know, coming from a function called &lt;code&gt;tf.constant&lt;/code&gt;. When we print the variable, we see that it returns a &lt;code&gt;tf.Tensor&lt;/code&gt; object, which is a pointer to the node that we just created. To emphasize this, here’s another example:&lt;/p&gt;

&lt;h6 id=&#34;code-1&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
another_two_node = tf.constant(2)
two_node = tf.constant(2)
tf.constant(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-2&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig2.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Every time we call &lt;code&gt;tf.constant&lt;/code&gt;, we create a new node in the graph. This is true even if the node is functionally identical to an existing node, even if we re-assign a node to the same variable, or  even if we don’t assign it to a variable at all.&lt;/p&gt;

&lt;p&gt;In contrast, if you make a new variable and set it equal to an existing node, you are just copying the pointer to that node and nothing is added to the graph:&lt;/p&gt;

&lt;h6 id=&#34;code-2&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
another_pointer_at_two_node = two_node
two_node = None
print two_node
print another_pointer_at_two_node
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-1&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;None
Tensor(&amp;quot;Const:0&amp;quot;, shape=(), dtype=int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-3&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig3.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Okay, let’s liven things up a bit:&lt;/p&gt;

&lt;h6 id=&#34;code-3&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node ## equivalent to tf.add(two_node, three_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-4&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig4.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Now we’re talking - that’s a bona-fide computational graph we got there! Notice that the &lt;code&gt;+&lt;/code&gt; operation is overloaded in Tensorflow, so adding two tensors together adds a node to the graph, even though it doesn’t seem like a Tensorflow operation on the surface.&lt;/p&gt;

&lt;p&gt;Okay, so &lt;code&gt;two_node&lt;/code&gt; points to a node containing 2, &lt;code&gt;three_node&lt;/code&gt; points to a node containing 3, and &lt;code&gt;sum_node&lt;/code&gt; points to a node containing…&lt;code&gt;+&lt;/code&gt;? What’s up with that? Shouldn’t it contain 5?&lt;/p&gt;

&lt;p&gt;As it turns out, no. Computational graphs contain only the steps of computation; they do not contain the results. At least…not yet!&lt;/p&gt;

&lt;h3 id=&#34;second-key-abstraction-the-session&#34;&gt;Second Key Abstraction: The Session&lt;/h3&gt;

&lt;p&gt;If there were March Madness for misunderstood TensorFlow abstractions, the session would be the #1 seed every year. It has that dubious honor due to being both unintuitively named and universally present &amp;ndash; nearly every Tensorflow program explicitly invokes &lt;code&gt;tf.Session()&lt;/code&gt; at least once.&lt;/p&gt;

&lt;p&gt;The role of the session is to handle the memory allocation and optimization that allows us to actually perform the computations specified by a graph. You can think of the computation graph as a “template” for the computations we want to do: it lays out all the steps. In order to make use of the graph, we also need to make a session, which allows us to actually do things; for example, going through the template node-by-node to allocate a bunch of memory for storing computation outputs. In order to do any computation with Tensorflow, you need both a graph and a session.&lt;/p&gt;

&lt;p&gt;The session contains a pointer to the global graph, which is constantly updated with pointers to all nodes. That means it doesn’t really matter whether you create the session before or after you create the nodes. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;After creating your session object, you can use &lt;code&gt;sess.run(node)&lt;/code&gt; to return the value of a node, and Tensorflow performs all computations necessary to determine that value.&lt;/p&gt;

&lt;h6 id=&#34;code-4&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
sess = tf.Session()
print sess.run(sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-2&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;5
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-5&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig4.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Wonderful! We can also pass a list, &lt;code&gt;sess.run([node1, node2,...])&lt;/code&gt;, and have it return multiple outputs:&lt;/p&gt;

&lt;h6 id=&#34;code-5&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
sess = tf.Session()
print sess.run([two_node, sum_node])
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-3&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[2, 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-6&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig4.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;In general, &lt;code&gt;sess.run()&lt;/code&gt; calls tend to be one of the biggest TensorFlow bottlenecks, so the fewer times you call it, the better. Whenever possible, return multiple items in a single &lt;code&gt;sess.run()&lt;/code&gt; call instead of making multiple calls.&lt;/p&gt;

&lt;h3 id=&#34;placeholders-feed-dict&#34;&gt;Placeholders &amp;amp; feed_dict&lt;/h3&gt;

&lt;p&gt;The computations we’ve done so far have been boring: there is no opportunity to pass in input, so they always output the same thing. A more worthwhile application might involve constructing a computation graph that takes in input, processes it in some (consistent) way, and returns an output.&lt;/p&gt;

&lt;p&gt;The most straightforward way to do this is with placeholders. A placeholder is a type of node that is designed to accept external input.&lt;/p&gt;

&lt;h6 id=&#34;code-6&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
input_placeholder = tf.placeholder(tf.int32)
sess = tf.Session()
print sess.run(input_placeholder)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-4&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Traceback (most recent call last):
...
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor &#39;Placeholder&#39; with dtype int32
	 [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=&amp;lt;unknown&amp;gt;, _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;]()]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-7&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig5.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;&amp;hellip;is a terrible example, since it throws an exception. Placeholders expect to be given a value. We didn’t supply one, so Tensorflow crashed.&lt;/p&gt;

&lt;p&gt;To provide a value, we use the feed_dict attribute of &lt;code&gt;sess.run()&lt;/code&gt;.&lt;/p&gt;

&lt;h6 id=&#34;code-7&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
input_placeholder = tf.placeholder(tf.int32)
sess = tf.Session()
print sess.run(input_placeholder, feed_dict={input_placeholder: 2})
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-5&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;2
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-8&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig5.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Much better. Notice the format of the dict passed into &lt;code&gt;feed_dict&lt;/code&gt;. The keys should be variables corresponding to placeholder nodes from the graph (which, as discussed earlier, really means &lt;em&gt;pointers&lt;/em&gt; to placeholder nodes in the graph). The corresponding values are the data elements to assign to each placeholder &amp;ndash; typically scalars or Numpy arrays.&lt;/p&gt;

&lt;h3 id=&#34;third-key-abstraction-computation-paths&#34;&gt;Third Key Abstraction: Computation Paths&lt;/h3&gt;

&lt;p&gt;Let’s try another example involving placeholders:&lt;/p&gt;

&lt;h6 id=&#34;code-8&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
input_placeholder = tf.placeholder(tf.int32)
three_node = tf.constant(3)
sum_node = input_placeholder + three_node
sess = tf.Session()
print sess.run(three_node)
print sess.run(sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-6&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;3
Traceback (most recent call last):
...
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor &#39;Placeholder_2&#39; with dtype int32
	 [[Node: Placeholder_2 = Placeholder[dtype=DT_INT32, shape=&amp;lt;unknown&amp;gt;, _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;]()]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-9&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig6.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Why does the second call to &lt;code&gt;sess.run()&lt;/code&gt; fail? And why does it raise an error related to &lt;code&gt;input_placeholder&lt;/code&gt;, even though we are not evaluating &lt;code&gt;input_placeholder&lt;/code&gt;? The answer lies in the final key Tensorflow abstraction: computation paths. Luckily, this one is very intuitive.&lt;/p&gt;

&lt;p&gt;When we call &lt;code&gt;sess.run()&lt;/code&gt; on a node that is dependent on other nodes in the graph, we need to compute the values of those nodes, too. And if those nodes have dependencies, we need to calculate those values (and so on and so on&amp;hellip;) until we reach the “top” of the computation graph where nodes have no predecessors.&lt;/p&gt;

&lt;p&gt;Consider the computation path of &lt;code&gt;sum_node&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig7.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;
&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig8.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;All three nodes need to be evaluated to compute the value of &lt;code&gt;sum_node&lt;/code&gt;. Crucially, this includes our un-filled placeholder and explains the exception!&lt;/p&gt;

&lt;p&gt;In contrast, consider the computation path of &lt;code&gt;three_node&lt;/code&gt;:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig9.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Due to the graph structure, we don’t need to compute all of the nodes in order to evaluate the one we want! Because we don’t need to evaluate &lt;code&gt;placeholder_node&lt;/code&gt; to evaluate &lt;code&gt;three_node&lt;/code&gt;, running &lt;code&gt;sess.run(three_node)&lt;/code&gt; doesn’t raise an exception.&lt;/p&gt;

&lt;p&gt;The fact that Tensorflow automatically routes computation only through nodes that are necessary is a huge strength of the framework. It saves a lot of runtime on calls if the graph is very big and has many nodes that are not necessary. It allows us to construct large, “multi-purpose” graphs, which use a single, shared set of core nodes to do different things depending on which computation path is taken. For almost every application, it’s important to think about &lt;code&gt;sess.run()&lt;/code&gt; calls in terms of the computation path taken.&lt;/p&gt;

&lt;h3 id=&#34;variables-side-effects&#34;&gt;Variables &amp;amp; Side Effects&lt;/h3&gt;

&lt;p&gt;So far, we’ve seen two types of “no-ancestor” nodes: &lt;code&gt;tf.constant&lt;/code&gt;, which is the same for every run, and &lt;code&gt;tf.placeholder&lt;/code&gt;, which is different for every run. There’s a third case that we often want to consider: a node which &lt;em&gt;generally&lt;/em&gt; has the same value between runs, but can also be updated to have a new value. That’s where variables come in.&lt;/p&gt;

&lt;p&gt;Understanding variables is essential to doing deep learning with Tensorflow, because the parameters of your model fall into this category. During training, you want to update your parameters at every step, via gradient descent; but during evaluation, you want to keep your parameters fixed, and pass a bunch of different test-set inputs into the model. More than likely, all of your model’s trainable parameters will be implemented as variables.&lt;/p&gt;

&lt;p&gt;To create variables, use &lt;code&gt;tf.get_variable()&lt;/code&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; The first two arguments to &lt;code&gt;tf.get_variable()&lt;/code&gt; are required; the rest are optional. They are &lt;code&gt;tf.get_variable(name, shape)&lt;/code&gt;. &lt;code&gt;name&lt;/code&gt; is a string which uniquely identifies this variable object. It must be unique relative to the global graph, so be careful to keep track of all names you have used to ensure there are no duplicates.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;code&gt;shape&lt;/code&gt; is an array of integers corresponding to the shape of a tensor; the syntax of this is intuitive &amp;ndash; just one integer per dimension, in order. For example, a 3x8 matrix would have shape &lt;code&gt;[3, 8]&lt;/code&gt;. To create a scalar, use an empty list as your shape: &lt;code&gt;[]&lt;/code&gt;.&lt;/p&gt;

&lt;h6 id=&#34;code-9&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
count_variable = tf.get_variable(&amp;quot;count&amp;quot;, [])
sess = tf.Session()
print sess.run(count_variable)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-7&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Traceback (most recent call last):
...
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value count
	 [[Node: _retval_count_0_0 = _Retval[T=DT_FLOAT, index=0, _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;](count)]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-10&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig10.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Alas, another exception. When a variable node is first created, it basically stores “null”, and any attempts to evaluate it will result in this exception. We can only evaluate a variable after putting a value into it first. There are two main ways to put a value into a variable: initializers and &lt;code&gt;tf.assign()&lt;/code&gt;. Let’s look at &lt;code&gt;tf.assign()&lt;/code&gt; first:&lt;/p&gt;

&lt;h6 id=&#34;code-10&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
count_variable = tf.get_variable(&amp;quot;count&amp;quot;, [])
zero_node = tf.constant(0.)
assign_node = tf.assign(count_variable, zero_node)
sess = tf.Session()
sess.run(assign_node)
print sess.run(count_variable)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-8&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;0
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-11&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig11.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;code&gt;tf.assign(target, value)&lt;/code&gt; is a node that has some unique properties compared to nodes we’ve seen so far:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Identity operation. &lt;code&gt;tf.assign(target, value)&lt;/code&gt; does not do any interesting computations, it is always just equal to &lt;code&gt;value&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Side effects. When computation “flows” through &lt;code&gt;assign_node&lt;/code&gt;, side effects happen to other things in the graph. In this case, the side effect is to replace the value of &lt;code&gt;count_variable&lt;/code&gt; with the value stored in &lt;code&gt;zero_node&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Non-dependent edges. Even though the &lt;code&gt;count_variable&lt;/code&gt; node and the &lt;code&gt;assign_node&lt;/code&gt; are connected in the graph, neither is dependent on the other. This means computation will not flow back through that edge when evaluating either node. However, &lt;code&gt;assign_node&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; dependent on &lt;code&gt;zero_node&lt;/code&gt;; it needs to know what to assign.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Side effect” nodes underpin most of the Tensorflow deep learning workflow, so make sure you really understand what’s going on here. When we call &lt;code&gt;sess.run(assign_node)&lt;/code&gt;, the computation path goes through &lt;code&gt;assign_node&lt;/code&gt; and &lt;code&gt;zero_node&lt;/code&gt;.&lt;/p&gt;

&lt;h6 id=&#34;graph-12&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig12.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;As computation flows through any node in the graph, it also enacts any side effects controlled by that node, shown in green. Due to the particular side effects of &lt;code&gt;tf.assign&lt;/code&gt;, the memory associated with &lt;code&gt;count_variable&lt;/code&gt; (which was previously “null”) is now permanently set to equal 0. This means that when we next call &lt;code&gt;sess.run(count_variable)&lt;/code&gt;, we don’t throw any exceptions. Instead, we get a value of 0. Success!&lt;/p&gt;

&lt;p&gt;Next, let’s look at initializers:&lt;/p&gt;

&lt;h6 id=&#34;code-11&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
const_init_node = tf.constant_initializer(0.)
count_variable = tf.get_variable(&amp;quot;count&amp;quot;, [], initializer=const_init_node)
sess = tf.Session()
print sess.run([count_variable])
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-9&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Traceback (most recent call last):
...
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value count
	 [[Node: _retval_count_0_0 = _Retval[T=DT_FLOAT, index=0, _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;](count)]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-13&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig13.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Okay, what happened here? Why didn’t the initializer work?&lt;/p&gt;

&lt;p&gt;The answer lies in the split between sessions and graphs. We’ve set the &lt;code&gt;initializer&lt;/code&gt; property of &lt;code&gt;get_variable&lt;/code&gt; to point at our &lt;code&gt;const_init_node&lt;/code&gt;, but that just added a new connection between nodes in the graph. We haven’t done anything about the root of the exception: &lt;em&gt;the memory associated with the variable node&lt;/em&gt; (which is stored in the session, not the graph!) is still set to “null”. We need the session to tell the &lt;code&gt;const_init_node&lt;/code&gt; to actually update the variable.&lt;/p&gt;

&lt;h6 id=&#34;code-12&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
const_init_node = tf.constant_initializer(0.)
count_variable = tf.get_variable(&amp;quot;count&amp;quot;, [], initializer=const_init_node)
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
print sess.run(count_variable)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-10&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;0.
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-14&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig14.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;To do this, we added another, special node: &lt;code&gt;init = tf.global_variables_initializer()&lt;/code&gt;. Similarly to &lt;code&gt;tf.assign()&lt;/code&gt;, this is a node with side effects. In contrast to &lt;code&gt;tf.assign()&lt;/code&gt;, we don’t actually need to specify what its inputs are! &lt;code&gt;tf.global_variables_initializer()&lt;/code&gt; will look at the global graph at the moment of its creation and automatically add dependencies to every &lt;code&gt;tf.initializer&lt;/code&gt; in the graph. When we then evaluate it with &lt;code&gt;sess.run(init)&lt;/code&gt;, it goes to each of the initializers and tells them to do their thang, initializing the variables and allowing us to run &lt;code&gt;sess.run(count_variable)&lt;/code&gt; without an error.&lt;/p&gt;

&lt;h4 id=&#34;variable-sharing&#34;&gt;Variable Sharing&lt;/h4&gt;

&lt;p&gt;You may encounter Tensorflow code with variable sharing, which involves creating a scope and setting “reuse=True”. I strongly recommend that you don’t use this in your own code. If you want to use a single variable in multiple places, simply keep track of your pointer to that variable&amp;rsquo;s node programmatically, and re-use it when you need to. In other words, you should have only a single call of &lt;code&gt;tf.get_variable()&lt;/code&gt; for each parameter you intend to store in memory.&lt;/p&gt;

&lt;h3 id=&#34;optimizers&#34;&gt;Optimizers&lt;/h3&gt;

&lt;p&gt;At last: on to the actual deep learning! If you’re still with me, the remaining concepts should be extremely straightforward.&lt;/p&gt;

&lt;p&gt;In deep learning, the typical “inner loop” of training is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get an input and true_output&lt;/li&gt;
&lt;li&gt;Compute a “guess” based on the input and your parameters&lt;/li&gt;
&lt;li&gt;Compute a “loss” based on the difference between your guess and the true_output&lt;/li&gt;
&lt;li&gt;Update the parameters according to the gradient of the loss&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s put together a quick script for a toy linear regression problem:&lt;/p&gt;

&lt;h6 id=&#34;code-13&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf

### build the graph
## first set up the parameters
m = tf.get_variable(&amp;quot;m&amp;quot;, [], initializer=tf.constant_initializer(0.))
b = tf.get_variable(&amp;quot;b&amp;quot;, [], initializer=tf.constant_initializer(0.))
init = tf.global_variables_initializer()

## then set up the computations
input_placeholder = tf.placeholder(tf.float32)
output_placeholder = tf.placeholder(tf.float32)

x = input_placeholder
y = output_placeholder
y_guess = m * x + b

loss = tf.square(y - y_guess)

## finally, set up the optimizer and minimization node
optimizer = tf.train.GradientDescentOptimizer(1e-3)
train_op = optimizer.minimize(loss)

### start the session
sess = tf.Session()
sess.run(init)

### perform the training loop
import random

## set up problem
true_m = random.random()
true_b = random.random()

for update_i in range(100000):
  ## (1) get the input and output
  input_data = random.random()
  output_data = true_m * input_data + true_b

  ## (2), (3), and (4) all take place within a single call to sess.run()!
  _loss, _ = sess.run([loss, train_op], feed_dict={input_placeholder: input_data, output_placeholder: output_data})
  print update_i, _loss

### finally, print out the values we learned for our two variables
print &amp;quot;True parameters:     m=%.4f, b=%.4f&amp;quot; % (true_m, true_b)
print &amp;quot;Learned parameters:  m=%.4f, b=%.4f&amp;quot; % tuple(sess.run([m, b]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-11&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;0 2.3205383
1 0.5792742
2 1.55254
3 1.5733259
4 0.6435648
5 2.4061265
6 1.0746256
7 2.1998715
8 1.6775116
9 1.6462423
10 2.441034
...
99990 2.9878322e-12
99991 5.158629e-11
99992 4.53646e-11
99993 9.422685e-12
99994 3.991829e-11
99995 1.134115e-11
99996 4.9467985e-11
99997 1.3219648e-11
99998 5.684342e-14
99999 3.007017e-11
True parameters:     m=0.3519, b=0.3242
Learned parameters:  m=0.3519, b=0.3242
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the loss goes down to basically nothing, and we wind up with a really good estimate of the true parameters. Hopefully, the only part of the code that is new to you is this segment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## finally, set up the optimizer and minimization node
optimizer = tf.train.GradientDescentOptimizer(1e-3)
train_op = optimizer.minimize(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But, now that you have a good understanding of the concepts underlying Tensorflow, this code is easy to explain! The first line, &lt;code&gt;optimizer = tf.train.GradientDescentOptimizer(1e-3)&lt;/code&gt;, is not adding a node to the graph. It is simply creating a Python object that has useful helper functions. The second line, &lt;code&gt;train_op = optimizer.minimize(loss)&lt;/code&gt;, is adding a node to the graph, and storing a pointer to it in variable &lt;code&gt;train_op&lt;/code&gt;. The &lt;code&gt;train_op&lt;/code&gt; node has no output, but has a very complicated side effect:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;train_op&lt;/code&gt; traces back through the computation path of its input, &lt;code&gt;loss&lt;/code&gt;, looking for variable nodes. For each variable node it finds, it computes the gradient of the loss with respect to that variable. Then, it computes a new value for that variable: the current value minus the gradient times the learning rate. Finally, it performs an assign operation to update the value of the variable.&lt;/p&gt;

&lt;p&gt;So essentially, when we call &lt;code&gt;sess.run(train_op)&lt;/code&gt;, it does a step of gradient descent on all of our variables for us. Of course, we also need to fill in the input and output placeholders with our feed_dict, and we also want to print the loss, because it’s handy for debugging.&lt;/p&gt;

&lt;h3 id=&#34;debugging-with-tf-print&#34;&gt;Debugging with &lt;code&gt;tf.Print&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;As you start doing more complicated things with Tensorflow, you’re going to want to debug. In general, it’s quite hard to inspect what’s going on inside a computation graph. You can’t use a regular Python print statement, because you never have access to the values you want to print &amp;ndash; they are locked away inside the &lt;code&gt;sess.run()&lt;/code&gt; call. To elaborate, suppose you want to inspect an intermediate value of a computation. Before the &lt;code&gt;sess.run()&lt;/code&gt; call, the intermediate values do not exist yet. But when the &lt;code&gt;sess.run()&lt;/code&gt; call returns, the intermediate values are gone!&lt;/p&gt;

&lt;p&gt;Let’s look at a simple example.&lt;/p&gt;

&lt;h6 id=&#34;code-14&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
sess = tf.Session()
print sess.run(sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-12&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This lets us see our overall answer, 5. But what if we want to inspect the intermediate values, &lt;code&gt;two_node&lt;/code&gt; and &lt;code&gt;three_node&lt;/code&gt;? One way to inspect the intermediate values is to add a return argument to &lt;code&gt;sess.run()&lt;/code&gt; that points at each of the intermediate nodes you want to inspect, and then, after it has been returned, print it.&lt;/p&gt;

&lt;h6 id=&#34;code-15&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
sess = tf.Session()
answer, inspection = sess.run([sum_node, [two_node, three_node]])
print inspection
print answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-13&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[2, 3]
5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This often works well, but as code becomes more complex, it can be a bit awkward. A more convenient approach is to use a &lt;code&gt;tf.Print&lt;/code&gt; statement. Confusingly, &lt;code&gt;tf.Print&lt;/code&gt; is actually a type of Tensorflow node, which has both output and side effects! It has two required arguments: a node to copy, and a list of things to print. The “node to copy” can be any node in the graph; &lt;code&gt;tf.Print&lt;/code&gt; is an identity operation with respect to its “node to copy”, meaning that it outputs an exact copy of its input. But, it also prints all the current values in the “list of things to print” as a side effect.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h6 id=&#34;code-16&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
print_sum_node = tf.Print(sum_node, [two_node, three_node])
sess = tf.Session()
print sess.run(print_sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-14&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[2][3]
5
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-15&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig15.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;One important, somewhat-subtle point about &lt;code&gt;tf.Print&lt;/code&gt;: printing is a side effect. Like all other side effects, printing only occurs if the computation flows through the &lt;code&gt;tf.Print&lt;/code&gt; node. If the &lt;code&gt;tf.Print&lt;/code&gt; node is not in the path of the computation, nothing will print. In particular, even if the original node that your &lt;code&gt;tf.Print&lt;/code&gt; node is copying is on the computation path, the &lt;code&gt;tf.Print&lt;/code&gt; node itself might not be. Watch out for this issue! When it strikes (and it eventually will), it can be incredibly frustrating if you aren’t specifically looking for it. As a general rule, try to always create your &lt;code&gt;tf.Print&lt;/code&gt; node immediately after creating the node that it copies.&lt;/p&gt;

&lt;h6 id=&#34;code-17&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
### this new copy of two_node is not on the computation path, so nothing prints!
print_two_node = tf.Print(two_node, [two_node, three_node, sum_node])
sess = tf.Session()
print sess.run(sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-15&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;5
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-16&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig16.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://wookayin.github.io/tensorflow-talk-debugging/#1&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a great resource which provides additional practical debugging advice.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Hopefully this post helped you get a better intuition for what Tensorflow is, how it works, and how to use it. At the end of the day, the concepts presented here are fundamental to all Tensorflow programs, but this is only scratching the surface. In your Tensorflow adventures, you will likely encounter all sorts of other fun things that you want to use: conditionals, iteration, distributed Tensorflow, variable scopes, saving &amp;amp; loading models, multi-graph, multi-session, and multi-core, data-loader queues, and much more. Many of these topics I will cover in future posts. But if you build on the ideas you learned here with the official documentation, some code examples, and just a pinch of deep learning magic, I’m sure you’ll be able to figure it out!&lt;/p&gt;

&lt;p&gt;For more detail on how these abstractions are implemented in Tensorflow, and how to interact with them, take a look at my &lt;a href=&#34;https://jacobbuckman.com/post/graph-inspection/&#34; target=&#34;_blank&#34;&gt;post on inspecting computational graphs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Please give me feedback in the comments (or via email) if anything discussed in this guide was unclear. And if you enjoyed this post, let me know what I should cover next!&lt;/p&gt;

&lt;p&gt;Happy training!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is the first of a series; click &lt;a href=&#34;https://jacobbuckman.com/post/tensorflow-the-confusing-parts-2/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for the next post, or &lt;a href=&#34;https://jacobbuckman.com/categories/tftcp/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for a list of all posts in this series.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Many thanks to Kathryn Rough, Katherine Lee, Sara Hooker, and Ludwig Schubert for all of their help and feedback when writing this post.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:0&#34;&gt;&lt;a href=&#34;https://docs.chainer.org/en/stable/guides/define_by_run.html&#34; target=&#34;_blank&#34;&gt;This page&lt;/a&gt; from the Chainer documentation describes the difference between define-and-run and define-by-run.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:0&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:1&#34;&gt;In general, I prefer to make sure I already have the entire graph in place when I create a session, and I follow that paradigm in my examples here. But you might see it done differently in other Tensorflow code.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Since the Tensorflow team is dedicated to backwards compatibility, there are several ways to create variables. In older code, it is common to also encounter the &lt;code&gt;tf.Variable()&lt;/code&gt; syntax, which serves the same purpose.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Name management can be made a bit easier with &lt;code&gt;tf.variable_scope()&lt;/code&gt;. I will cover scoping in more detail In a future post!
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Note that &lt;code&gt;tf.Print&lt;/code&gt; is not compatible with Colab or IPython notebooks; it prints to the standard output, which is not shown in the notebook. There are various solutions on StackOverflow.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is Generator Conditioning Causally Related to GAN Performance?</title>
      <link>https://jacobbuckman.com/publication/jacobean-gan/</link>
      <pubDate>Tue, 12 Jun 2018 12:27:43 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/publication/jacobean-gan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Lattice Language Models</title>
      <link>https://jacobbuckman.com/publication/nllm/</link>
      <pubDate>Sun, 03 Jun 2018 12:27:16 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/publication/nllm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Thermometer Encoding: One Hot Way to Resist Adversarial Examples</title>
      <link>https://jacobbuckman.com/publication/thermometer/</link>
      <pubDate>Wed, 25 Apr 2018 12:26:57 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/publication/thermometer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transition-Based Dependency Parsing with Heuristic Backtracking</title>
      <link>https://jacobbuckman.com/publication/heuristic-backtracking/</link>
      <pubDate>Fri, 25 Nov 2016 12:26:43 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/publication/heuristic-backtracking/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
