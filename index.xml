<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Buckman&#39;s Homepage on Buckman&#39;s Homepage</title>
    <link>https://jacobbuckman.com/</link>
    <description>Recent content in Buckman&#39;s Homepage on Buckman&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>OpenAI Five Takeaways</title>
      <link>https://jacobbuckman.com/post/openaifive-takeaways/</link>
      <pubDate>Mon, 06 Aug 2018 18:38:24 -0400</pubDate>
      
      <guid>https://jacobbuckman.com/post/openaifive-takeaways/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://blog.openai.com/openai-five-benchmark-results/&#34; target=&#34;_blank&#34;&gt;On August 5th, OpenAI successfully defeated top human players in a Dota 2 best-of-three series&lt;/a&gt;.
Their AI Dota agent, called OpenAI Five, was a deep neural network trained using reinforcement learning.
As a researcher studying deep reinforcement learning, as well as a long-time follower of competitive Dota 2, I found this face-off really interesting.
The eventual OAI5 victory was both impressive and well-earned - congrats to the team at OpenAI, and props to the humans for a hard-fought battle!
Of course, this result has provoked a lot of discussion; here&amp;rsquo;s my thoughts.&lt;/p&gt;

&lt;h3 id=&#34;how-did-openai-five-win&#34;&gt;How did OpenAI Five win?&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m sure this series will be analyzed by people with far deeper understanding of Dota than me, but in my opinion, OpenAI Five essentially won on the back of its teamfighting ability.
Good positioning and well-coordinated ability usage made it almost impossible to take 5v5 fights against the bot once it got started pushing.
During the laning phase, both teams were able to find pickoffs, and the human team actually pulled ahead in farm.
During the mid-game, whenever the bot team was not actively pushing, the humans were more efficient in getting resources from around the map.
Many times throughout the game, OAI5 did actions that were almost unequivocally bad, such as the pointless Smoke of Deceit usages we saw a couple of times.
But once OAI5 formed up a 5-man squad and started to push, the humans were steamrolled.&lt;/p&gt;

&lt;p&gt;This plan was followed to a t on both of the first two games, leading to decisive OAI5 victories.
During game three, it looked like the bots wanted to try the same thing again, but after their poor hero composition led to lost fights, they began simply doing anything they could to extend the game.
It certainly seems reasonable to me to assert that OAI5 was able to discover a near-unbeatable strategy, and learned to execute it perfectly.
In my opinion, these are the two main interesting outcomes of this result.
I&amp;rsquo;d like to describe why I find them surprising, and highlight some caveats.&lt;/p&gt;

&lt;h3 id=&#34;discovering-an-unbeatable-strategy&#34;&gt;Discovering an unbeatable strategy&lt;/h3&gt;

&lt;p&gt;Dota 2 has massive state and action spaces - according to &lt;a href=&#34;https://blog.openai.com/openai-five/&#34; target=&#34;_blank&#34;&gt;an earlier blog post&lt;/a&gt;, the dimensionalities of the state and action vectors are 20,000 and 1,000, respectively.
Right from the start, there is a combinatorial explosion in observed state stemming from team compositions; these quickly grow even more differentiated based on skill and item builds.
Further complicating things, the action space is a mix of discrete choices (i.e. whether to move or use a skill, what skill to use) and continuous parameters (i.e. where to move).
This results in an enormous set of states that need to be explored and remembered, and a potentially huge amount of promising actions in each one.&lt;/p&gt;

&lt;p&gt;When I first heard about OpenAI tackling Dota 2, I assumed that exploration would be one of the major challenges.
Surely standard noise-based or epsilon-greedy approaches would fall short - in this insanely large state space, what are the chances that it will stumble upon good gameplay?&lt;/p&gt;

&lt;p&gt;But in the absence of an explicit comment in any press release about exploration, I assume that OpenAI&amp;rsquo;s exploration techniques were, in fact, totally standard.
It seems that through nothing but the reward engineering efforts of the members of the development team, OAI5 was able to get a dense enough reward signal that it discovered a near-unbeatable strategy.
This is pretty surprising so me: I&amp;rsquo;m a big believer in the end-to-end RL dream (i.e. training exclusively from the win/loss reward signal under the assumption that any human intuition we add to the training process will &amp;ldquo;dilute&amp;rdquo; the purity of the solution discovered with our pesky suboptimalities).
My thinking was that in any real-world problem, effective reward engineering would be so difficult as to be impossible.
&lt;a href=&#34;https://www.youtube.com/watch?v=tlOIHko8ySg&#34; target=&#34;_blank&#34;&gt;There are many examples of reward-engineering-gone-wrong in various domains&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Those in favor of reward engineering argue that it&amp;rsquo;s possible to avoid these issues with careful engineering.
It&amp;rsquo;s true that an AI trained in an environment that rewards last-hitting is unlikely to ever learn a revolutionary new strategy that ignores last-hits entirely.
But the chance that such a strategy exists seems quite low, and it&amp;rsquo;s clearly possible to learn a strategy that &lt;em&gt;both&lt;/em&gt; secures last-hits and wins games.
So, why not do it? These &amp;ldquo;reward breadcrumbs&amp;rdquo; will guide us towards certain areas of the solution space and away from others.
But as long as we are careful to ensure that the getting the engineered reward is at least compatible with overall success on the task, it may be an indispensible part of solving real-world RL problems.&lt;/p&gt;

&lt;p&gt;Dota 2 is the first example I&amp;rsquo;ve seen of this in practice; a challenging real-world (ish) task with a seemingly impossible exploration issue, that was overcome by clever reward engineering.
Maybe it&amp;rsquo;s true that another agent, trained in the exact same way but with just the win/loss reward, could eventually learn a better strategy than OpenAI Five.
But until someone manages to make one, I don&amp;rsquo;t see much reason to believe that.
And it seems likely that even if such a system existed, it would almost certainly take a lot longer to train.
So seeing this result has begun to win me over in favor of reward engineering on real-world tasks.&lt;/p&gt;

&lt;h4 id=&#34;caveats&#34;&gt;Caveats&lt;/h4&gt;

&lt;p&gt;Of course, this would likely have still been inadequate without scale.
In the millions upon millions of games played by the system, even naive exploration is able to cover an enormous amount of possible game-states.
And as massive as the current state and action space are, they are tiny compared to the eventual number of states once the remainder of the features are added.
The remaining ~100 heros will increase the state space exponentially, and adding in the ability to control multiple units (via illusions, summons, etc.) will increase the action space exponentially as well: in addition to choosing an action, the agent must choose between the $2^n$ potential subsets of controllable units at its disposal.
It remains to be seen whether OpenAI&amp;rsquo;s current exploration strategies will be able to keep up.&lt;/p&gt;

&lt;p&gt;Additionally, the very existence of an &amp;ldquo;unbeatable strategy&amp;rdquo; undermines some of the key reasons that Dota 2 was selected as an interesting platform for research.
For example, people often cite strategic decision making, long-term strategy, and complex opponent modeling and counter-play as fundamental to successful Dota.
But if an unbeatable strategy exists, that goes out the window.
Simply executing on a known strategy doesn&amp;rsquo;t require any higher-level reasoning or planning, and certainly doesn&amp;rsquo;t require much opponent modeling.
Framed in this way, I see these Dota victories as being in the same category as &lt;a href=&#34;https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/&#34; target=&#34;_blank&#34;&gt;learning to walk in a complex, noisy environment&lt;/a&gt;.
&amp;ldquo;Just do your thing, Dota bot, and do your best to correct for whatever those silly human opponents throw at you.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s entirely possible that OAI5 &lt;em&gt;would&lt;/em&gt; learn these higher-level behaviors, if trained to play a more balanced game.
Dota 2 is balanced around all 115 heroes and all items, but the version played by OAI5 uses a pool of only 18, and so any concept of balance goes out the window.
As Blitz said during the post-game panel discussion: the limited set of available heroes forced the human players to &amp;ldquo;play the bot&amp;rsquo;s game.&amp;rdquo;
Heroes with strong counter-push and teamfight like Venomancer, Enigma, and Naga Siren would have directly countered the strategy played by OAI5, and &amp;ldquo;backdoor&amp;rdquo; heroes like Furion and Lycan would have allowed the humans to play around the bots and gain advantage in many lanes at once.
Once these heroes are added to the pool, perhaps we will see a totally different set of strategies.&lt;/p&gt;

&lt;p&gt;(Also, Icefrog does his best, but there is no guarantee that even the full game of Dota 2 is balanced!
The game is constantly being tweaked to make more and more strategies viable.
Alliance&amp;rsquo;s nearly-undefeated run at TI3 on the back of a pushing strategy - somewhat similar to OAI5&amp;rsquo;s, in fact - led to a swift nerf of the heroes and items responsible.
And of course, who can forget the &amp;ldquo;Ho Ho Ha Ha&amp;rdquo; patch of 6.83, where picking Sniper or Troll Warlord would basically guarantee your team a victory.
So if a future OAI5, trained on the full game, is able to come up with a single, dominant strategy, expect that strategy to be patched into uselessness soon afterwards.)&lt;/p&gt;

&lt;h3 id=&#34;executing-perfectly-against-an-out-of-domain-opponent&#34;&gt;Executing perfectly against an out-of-domain opponent&lt;/h3&gt;

&lt;p&gt;In spite of some good-natured ribbing from the casters, I thought that OAI5 did astonishingly well in dealing with situations that is was not exposed to in training.
The concept of &amp;ldquo;pulling creeps&amp;rdquo; was not something that it had ever learned to do, and therefore also not something that it had learned to deal with.
But the bot handled the sudden lack of creep wave more-or-less as a surprised human would.
In the second game of the series, it was even able to capitalize on a pull that it had vision of and pick off a hero.&lt;/p&gt;

&lt;h4 id=&#34;caveats-1&#34;&gt;Caveats&lt;/h4&gt;

&lt;p&gt;Since this was only a three-game series, and the humans played mostly classic non-cheesy Dota 2, it&amp;rsquo;s hard to get a sense of just how stable the AI is when encountering unfamiliar situations.
But from the few games we&amp;rsquo;ve seen, I think it looks much less exploitable than its 1v1 counterpart.
The flipside of having to explore such a vast state space is that it seemingly becomes a lot harder to force the bot into a state that is &lt;em&gt;truly&lt;/em&gt; unexpected.
Even if some minor things are &amp;ldquo;off&amp;rdquo;, there are enough familiar elements that OAI5 is able to stick to its plan.
Whether this is a product of domain randomization and self-play, or simply a reflection of the increased importance of static objectives like towers in 5v5, I&amp;rsquo;m not certain.
(Or whether it is even a real effect - if OpenAI sets up a public free-for-all like they did last year, it might prove fragile after all!)&lt;/p&gt;

&lt;h3 id=&#34;on-cooperation&#34;&gt;On cooperation&lt;/h3&gt;

&lt;p&gt;The teamfight coordination displayed by OpenAI is absolutely incredible by human standards.
In humans, this level of teamfight requires months of practice and an intense focus on cooperation; it is reserved for the highest-level professional teams.
And yet, I don&amp;rsquo;t think that the various heroes controlled by the bot displayed anything resembling human cooperation.
When we say that two agents to are &amp;ldquo;cooperating&amp;rdquo;, I think an essential part of that definition involves reasoning under uncertainty.&lt;/p&gt;

&lt;p&gt;In humans playing Dota 2, I see two major sources of uncertainty: policy uncertainty and information uncertainty.&lt;/p&gt;

&lt;p&gt;To illustrate what I mean by policy uncertainty, consider what it would be like to be placed on a team with a group of random Dota players.
You don&amp;rsquo;t know exactly who the other players are, what they will do, how they will react.
At first, you must behave cautiously; you have some assumptions about what they will do, but you need to make sure you always have a backup plan in case they deviate.
As you play with them more, and get to know them better, you get a better sense of their actions, and they of yours.
Eventually, you can very reliably predict their actions, and trust them to have your back when needed.
But at the end of the day, there&amp;rsquo;s always still some uncertainty there.&lt;/p&gt;

&lt;p&gt;Information uncertainty arises when two players have a different subset of the information on the screen visible to them at any given time.
Though all human players can scroll their screen around to see everything visible to all team members, in practice, there is too much going on at any given time for a single player to absorb it all.
Therefore, even at the same moment of the same game, different players are likely to have taken different subsets of the available information as input.
Since it&amp;rsquo;s impossible to know exactly what each of your teammates was looking at, and they might take various actions depending on what they saw, this is a second major source of uncertainty.&lt;/p&gt;

&lt;p&gt;OAI5 is, by design, able to skirt these issues.
The network controlling each hero can take in the full state of the map at every tick, meaning that all heroes have access to identical information.
And since all of the millions of games played have had the same &amp;ldquo;player&amp;rdquo; on each hero, there is essentially no policy uncertainty - each hero can perfectly predict the actions of each other.
With neither policy nor information differences, OAI5 is able to display &amp;ldquo;perfect cooperation&amp;rdquo;, essentially controlling all five heroes as though they were a single entity.&lt;/p&gt;

&lt;p&gt;This is excellent for Dota bots, but I think it&amp;rsquo;s disingenuous to use the word &amp;ldquo;cooperation&amp;rdquo; to describe it, because it doesn&amp;rsquo;t generalize well to other settings where cooperation is important.
For one thing, I&amp;rsquo;d wager that OAI5 would be extremely crippled if any one of its heroes were replaced by a human player, of &lt;em&gt;any&lt;/em&gt; skill level.
Even if the 5th player is a top pro, playing excellently, he or she would still be playing &lt;em&gt;differently&lt;/em&gt; than expected, and the bot would be unable to cooperate as a result.
(I&amp;rsquo;d love to see OpenAI try this and let me know what the result is, though!)&lt;/p&gt;

&lt;p&gt;I think that this &amp;ldquo;perfect cooperation&amp;rdquo; is the Dota equivalent of &amp;ldquo;aimbotting&amp;rdquo; in FPS games - there&amp;rsquo;s a fundamental assumption about the limitations of human beings built into the game, and all top humans reach this physical limit, which means they are forced to compete with one another by using high-level reasioning and strategy.
By sidestepping that limitation, a bot is able to easily win without any of the interesting stuff.
I would be much more convinced that the bots are using high-level strategy and long-term planning if they were handicapped in a way that brought them back down to the level of humans in this regard.&lt;/p&gt;

&lt;p&gt;To incorporate policy uncertainty, I&amp;rsquo;d like to see OpenAI train an ensemble of many OAI5 agents.
In any given 5v5 game, 10 agents would be randomly sampled and assigned to the ten heroes; each agent only learns from games that it participates in.
This would force the agents to have some uncertainty about how their teammates will behave, since they get different teammates on different games.
And since the ensemble is just a bunch of copies of the agent that they&amp;rsquo;ve already made, it should be somewhat straightforward for OpenAI to set up.
(Of course, the implementation difficulty depends a bit on the architecture OpenAI used; if there is a lot of parameter sharing between agents, this will be hard.)&lt;/p&gt;

&lt;p&gt;Incorporating information uncertainty is less of an issue, in my opinion; the bot&amp;rsquo;s ability to take in all available information on the map seems like more of a &amp;ldquo;fair&amp;rdquo; advantage.
It could maybe be incorporated by randomly masking out certain segments of the input at every frame, perhaps dropping out inputs further from the agents at a higher rate.&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Well, that ended up being a bit more of a brain dump than I intended.
In summary, I think that this is a super impressive accomplishment, and says a lot about how good policy search, domain randomization, and reward shaping are at finding solutions.
However, I&amp;rsquo;m hesitant to draw any conclusions about whether the solution found involves much higher-level reasoning or long-term planning.
Thanks for reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sample-efficient Reinforcement Learning with Stochastic Ensemble Value Expansion</title>
      <link>https://jacobbuckman.com/publication/steve/</link>
      <pubDate>Mon, 06 Aug 2018 02:00:24 -0400</pubDate>
      
      <guid>https://jacobbuckman.com/publication/steve/</guid>
      <description></description>
    </item>
    
    <item>
      <title>More on Graph Inspection</title>
      <link>https://jacobbuckman.com/post/graph-inspection/</link>
      <pubDate>Sun, 05 Aug 2018 01:39:42 -0400</pubDate>
      
      <guid>https://jacobbuckman.com/post/graph-inspection/</guid>
      <description>

&lt;p&gt;In &lt;a href=&#34;https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/&#34; target=&#34;_blank&#34;&gt;Tensorflow: The Confusing Parts (1)&lt;/a&gt;, I described the abstractions underlying Tensorflow at a high level in an intuitive manner. In this follow-up post, I dig more deeply, and examine how these abstractions are actually implemented. Understanding these implementation details isn&amp;rsquo;t necessarily essential to writing and using Tensorflow, but it allows us to inspect and debug computational graphs.&lt;/p&gt;

&lt;h2 id=&#34;inspecting-graphs&#34;&gt;Inspecting Graphs&lt;/h2&gt;

&lt;p&gt;The computational graph is not just a nebulous, immaterial abstraction; it is a computational object that exists, and can be inspected. Complicated graphs are difficult to debug if we are representing them entirely in our heads, but inspecting and debugging the actual graph object makes thigs much easier.&lt;/p&gt;

&lt;p&gt;To access the graph object, use &lt;code&gt;tf.get_default_graph()&lt;/code&gt;, which returns a pointer to the global default graph object:&lt;/p&gt;

&lt;h6 id=&#34;code&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
g = tf.get_default_graph()
print g
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;lt;tensorflow.python.framework.ops.Graph object at 0x1144ffd90&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This object has the potential to tell us everything we need to know about the computational graph we have constructed. But only if we know how to use it! First, let&amp;rsquo;s take a step back and dive a bit deeper into something I glossed over in the first part: the difference between edges and nodes.&lt;/p&gt;

&lt;p&gt;The mathematical definition of a graph includes both edges and nodes. A Tensorflow graph is no exception: it has &lt;code&gt;tf.Operation&lt;/code&gt; objects (nodes) and &lt;code&gt;tf.Tensor&lt;/code&gt; objects (edges). An operation results in a single tensor (edge) as an output, so it&amp;rsquo;s fine to conflate the two in most cases; that&amp;rsquo;s what I did in TFTCP1. But in terms of the actual Python objects that make up the graph, they are programmatically distinct.&lt;/p&gt;

&lt;p&gt;When we create a new node, there are actually three things happening under the hood:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We gather up all the &lt;code&gt;tf.Tensor&lt;/code&gt; objects corresponding to the incoming edges for our new node&lt;/li&gt;
&lt;li&gt;We create a new node, which is a &lt;code&gt;tf.Operation&lt;/code&gt; object&lt;/li&gt;
&lt;li&gt;We create one or more new outgoing edges, which are &lt;code&gt;tf.Tensor&lt;/code&gt; objects, and return pointers to them&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are three primary ways that we can inspect the graph to understand how these pieces fit together:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;List All Nodes:&lt;/strong&gt; &lt;code&gt;tf.Graph.get_operations()&lt;/code&gt; returns all operations in the graph&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inspecting Nodes:&lt;/strong&gt; &lt;code&gt;tf.Operation.inputs&lt;/code&gt; and &lt;code&gt;tf.Operation.outputs&lt;/code&gt; each return a list of &lt;code&gt;tf.Tensor&lt;/code&gt; objects, which correspond to the incoming edges and outgoing edges, respectively&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inspecting Edges:&lt;/strong&gt; &lt;code&gt;tf.Tensor.op&lt;/code&gt; returns a single &lt;code&gt;tf.Operation&lt;/code&gt; for which this tensor is the output, and &lt;code&gt;tf.Tensor.consumers()&lt;/code&gt; returns a list of all &lt;code&gt;tf.Operations&lt;/code&gt; for which this tensor is used as an input.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here&amp;rsquo;s an example of these in action:&lt;/p&gt;

&lt;h6 id=&#34;code-1&#34;&gt;Code&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.constant(2, name=&#39;a&#39;)
b = tf.constant(3, name=&#39;b&#39;)
c = a + b

print &amp;quot;Our tf.Tensor objects:&amp;quot;
print a
print b
print c
print

a_op = a.op
b_op = b.op
c_op = c.op

print &amp;quot;Our tf.Operation objects, printed in compressed form:&amp;quot;
print a_op.__repr__()
print b_op.__repr__()
print c_op.__repr__()
print

print &amp;quot;The default behavior of printing a tf.Operation object is to pretty-print:&amp;quot;
print c_op

print &amp;quot;Inspect consumers for each tensor:&amp;quot;
print a.consumers()
print b.consumers()
print c.consumers()
print

print &amp;quot;Inspect input tensors for each op:&amp;quot;
# it&#39;s in a weird format, tensorflow.python.framework.ops._InputList, so we need to convert to list() to inspect
print list(a_op.inputs)
print list(b_op.inputs)
print list(c_op.inputs)
print

print &amp;quot;Inspect input tensors for each op:&amp;quot;
print a_op.outputs
print b_op.outputs
print c_op.outputs
print

print &amp;quot;The list of all nodes (tf.Operations) in the graph:&amp;quot;
g = tf.get_default_graph()
ops_list = g.get_operations()
print ops_list
print

print &amp;quot;The list of all edges (tf.Tensors) in the graph, by way of list comprehension:&amp;quot;
tensors_list = [tensor for op in ops_list for tensor in op.outputs]
print tensors_list
print

print &amp;quot;Note that these are the same pointers we can find by referring to our various graph elements directly:&amp;quot;
print ops_list[0] == a_op, tensors_list[0] == a
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-1&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Our tf.Tensor objects:
Tensor(&amp;quot;a:0&amp;quot;, shape=(), dtype=int32)
Tensor(&amp;quot;b:0&amp;quot;, shape=(), dtype=int32)
Tensor(&amp;quot;add:0&amp;quot;, shape=(), dtype=int32)

Our tf.Operation objects, printed in compressed form:
&amp;lt;tf.Operation &#39;a&#39; type=Const&amp;gt;
&amp;lt;tf.Operation &#39;b&#39; type=Const&amp;gt;
&amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;

The default behavior of printing a tf.Operation object is to pretty-print:
name: &amp;quot;add&amp;quot;
op: &amp;quot;Add&amp;quot;
input: &amp;quot;a&amp;quot;
input: &amp;quot;b&amp;quot;
attr {
  key: &amp;quot;T&amp;quot;
  value {
    type: DT_INT32
  }
}

Inspect consumers for each tensor:
[&amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;]
[&amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;]
[]

Inspect input tensors for each op:
[]
[]
[&amp;lt;tf.Tensor &#39;a:0&#39; shape=() dtype=int32&amp;gt;, &amp;lt;tf.Tensor &#39;b:0&#39; shape=() dtype=int32&amp;gt;]

Inspect input tensors for each op:
[&amp;lt;tf.Tensor &#39;a:0&#39; shape=() dtype=int32&amp;gt;]
[&amp;lt;tf.Tensor &#39;b:0&#39; shape=() dtype=int32&amp;gt;]
[&amp;lt;tf.Tensor &#39;add:0&#39; shape=() dtype=int32&amp;gt;]

The list of all nodes (tf.Operations) in the graph:
[&amp;lt;tf.Operation &#39;a&#39; type=Const&amp;gt;, &amp;lt;tf.Operation &#39;b&#39; type=Const&amp;gt;, &amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;]

The list of all edges (tf.Tensors) in the graph, by way of list comprehension:
[&amp;lt;tf.Tensor &#39;a:0&#39; shape=() dtype=int32&amp;gt;, &amp;lt;tf.Tensor &#39;b:0&#39; shape=() dtype=int32&amp;gt;, &amp;lt;tf.Tensor &#39;add:0&#39; shape=() dtype=int32&amp;gt;]

Note that these are the same pointers we can find by referring to our various graph elements directly:
True True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a couple of funky things that we have to do to make everything nice to look at, but once you get used to them, inspecting the graph becomes second nature.&lt;/p&gt;

&lt;p&gt;Of course, no discussion of graphs would be complete without taking a look at &lt;code&gt;tf.Variable&lt;/code&gt; objects too:&lt;/p&gt;

&lt;h6 id=&#34;code-2&#34;&gt;Code&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
a = tf.constant(2, name=&#39;a&#39;)
b = tf.get_variable(&#39;b&#39;, [], dtype=tf.int32)
c = a + b

g = tf.get_default_graph()
ops_list = g.get_operations()
print

print &amp;quot;tf.Variable objects are really a bundle of four operations (and their corresponding tensors):&amp;quot;
print b
print ops_list
print

print &amp;quot;Two of these are accessed via their tf.Operations,&amp;quot;,
print &amp;quot;the core&amp;quot;, b.op.__repr__(), &amp;quot;and the initializer&amp;quot;, b.initializer.__repr__()
print &amp;quot;The other two are accessed via their tf.Tensors,&amp;quot;,
print &amp;quot;the initial-value&amp;quot;, b.initial_value, &amp;quot;and the current-value&amp;quot;, b.value()
print

print &amp;quot;A tf.Variable core-op takes no inputs, and outputs a tensor of type *_ref:&amp;quot;
print b.op.__repr__()
print list(b.op.inputs), b.op.outputs
print

print &amp;quot;A tf.Variable current-value is the output of a \&amp;quot;/read\&amp;quot; operation, which converts from *_ref to a tensor with a concrete data-type.&amp;quot;
print &amp;quot;Other ops use the concrete node as their input:&amp;quot;
print b.value()
print b.value().op.__repr__()
print list(c.op.inputs)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-2&#34;&gt;Output&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.Variable objects are really a bundle of four operations (and their corresponding tensors):
&amp;lt;tf.Variable &#39;b:0&#39; shape=() dtype=int32_ref&amp;gt;
[&amp;lt;tf.Operation &#39;a&#39; type=Const&amp;gt;, &amp;lt;tf.Operation &#39;b/Initializer/zeros&#39; type=Const&amp;gt;, &amp;lt;tf.Operation &#39;b&#39; type=VariableV2&amp;gt;, &amp;lt;tf.Operation &#39;b/Assign&#39; type=Assign&amp;gt;, &amp;lt;tf.Operation &#39;b/read&#39; type=Identity&amp;gt;, &amp;lt;tf.Operation &#39;add&#39; type=Add&amp;gt;]

Two of these are accessed via their tf.Operations, the core &amp;lt;tf.Operation &#39;b&#39; type=VariableV2&amp;gt; and the initializer &amp;lt;tf.Operation &#39;b/Assign&#39; type=Assign&amp;gt;
The other two are accessed via their tf.Tensors, the initial-value Tensor(&amp;quot;b/Initializer/zeros:0&amp;quot;, shape=(), dtype=int32) and the current-value Tensor(&amp;quot;b/read:0&amp;quot;, shape=(), dtype=int32)

A tf.Variable core-op takes no inputs, and outputs a tensor of type *_ref:
&amp;lt;tf.Operation &#39;b&#39; type=VariableV2&amp;gt;
[] [&amp;lt;tf.Tensor &#39;b:0&#39; shape=() dtype=int32_ref&amp;gt;]

A tf.Variable current-value is the output of a &amp;quot;/read&amp;quot; operation, which converts from *_ref to a tensor with a concrete data-type.
Other ops use the concrete node as their input:
Tensor(&amp;quot;b/read:0&amp;quot;, shape=(), dtype=int32)
&amp;lt;tf.Operation &#39;b/read&#39; type=Identity&amp;gt;
[&amp;lt;tf.Tensor &#39;a:0&#39; shape=() dtype=int32&amp;gt;, &amp;lt;tf.Tensor &#39;b/read:0&#39; shape=() dtype=int32&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So a &lt;code&gt;tf.Variable&lt;/code&gt; adds (at least) four ops, but most of the details can be happily abstracted away by the &lt;code&gt;tf.Variable&lt;/code&gt; interface. In general, you can just assume that a &lt;code&gt;tf.Variable&lt;/code&gt; will be the thing you want it to be in any given circumstance. For example, if you want to assign a value to a variable, it will resolve to the core-op; if you want to use the variable in a computation, it will resolve to the current-value-op; etc.&lt;/p&gt;

&lt;p&gt;Take some time to play around with inspecting simple Tensorflow graphs in a Colab or interpreter - it will pay off in time saved debugging later!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow: The Confusing Parts (1)</title>
      <link>https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/</link>
      <pubDate>Mon, 25 Jun 2018 14:53:44 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;#understanding-tensorflow&#34;&gt;Click here to skip the intro and dive right in!&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;h3 id=&#34;what-is-this-who-are-you&#34;&gt;What is this? Who are you?&lt;/h3&gt;

&lt;p&gt;I’m Jacob, a &lt;a href=&#34;https://ai.google/research/join-us/ai-residency/&#34; target=&#34;_blank&#34;&gt;Google AI Resident&lt;/a&gt;. When I started the residency program in the summer of 2017, I had a lot of experience programming, and a good understanding of machine learning, but I had never used Tensorflow before. I figured that given my background I’d be able to pick it up quickly.  To my surprise, the learning curve was fairly steep, and even months into the residency, I would occasionally find myself confused about how to turn ideas into Tensorflow code. I’m writing this blog post as a message-in-a-bottle to my former self: it’s the introduction that I wish I had been given before starting on my journey. Hopefully, it will also be a helpful resource for others.&lt;/p&gt;

&lt;h3 id=&#34;what-was-missing&#34;&gt;What was missing?&lt;/h3&gt;

&lt;p&gt;In the three years since its release, &lt;a href=&#34;https://github.com/thedataincubator/data-science-blogs/blob/master/output/DL_libraries_final_Rankings.csv&#34; target=&#34;_blank&#34;&gt;Tensorflow has cemented itself as a cornerstone of the deep learning ecosystem&lt;/a&gt;. However, it can be non-intuitive for beginners, especially compared to define-by-run&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:0&#34;&gt;&lt;a href=&#34;#fn:0&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; neural network libraries like &lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34;&gt;PyTorch&lt;/a&gt; or &lt;a href=&#34;dynet.io&#34; target=&#34;_blank&#34;&gt;DyNet&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Many introductory Tensorflow tutorials exist, for doing everything from &lt;a href=&#34;https://www.tensorflow.org/tutorials/wide&#34; target=&#34;_blank&#34;&gt;linear regression&lt;/a&gt;, to &lt;a href=&#34;https://www.tensorflow.org/tutorials/layers&#34; target=&#34;_blank&#34;&gt;classifying MNIST&lt;/a&gt;, to &lt;a href=&#34;https://www.tensorflow.org/tutorials/seq2seq&#34; target=&#34;_blank&#34;&gt;machine translation&lt;/a&gt;. These concrete, practical guides are great resources for getting Tensorflow projects up and running, and can serve as jumping-off points for similar projects. But for the people who are working on applications for which a good tutorial does not exist, or who want to do something totally off the beaten path (as is common in research), Tensorflow can definitely feel frustrating at first.&lt;/p&gt;

&lt;p&gt;This post is my attempt to fill this gap. Rather than focusing on a specific task, I take a more general approach, and explain the fundamental abstractions underpinning Tensorflow. With a good grasp of these concepts, deep learning with Tensorflow becomes intuitive and straightforward.&lt;/p&gt;

&lt;h3 id=&#34;target-audience&#34;&gt;Target Audience&lt;/h3&gt;

&lt;p&gt;This tutorial is intended for people who already have some experience with both programming and machine learning, and want to pick up Tensorflow. For example: a computer science student who wants to use Tensorflow in the final project of her ML class; a software engineer who has just been assigned to a project that involves deep learning; or a bewildered new Google AI Resident (shout-out to past Jacob). If you’d like a refresher on the basics, &lt;a href=&#34;https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; &lt;a href=&#34;http://colah.github.io/&#34; target=&#34;_blank&#34;&gt;are&lt;/a&gt; &lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning--ud120&#34; target=&#34;_blank&#34;&gt;some&lt;/a&gt; &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34; target=&#34;_blank&#34;&gt;resources&lt;/a&gt;. Otherwise: let’s get started!&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;understanding-tensorflow&#34;&gt;Understanding Tensorflow&lt;/h1&gt;

&lt;h3 id=&#34;tensorflow-is-not-a-normal-python-library&#34;&gt;Tensorflow Is Not A Normal Python Library&lt;/h3&gt;

&lt;p&gt;Most Python libraries are written to be natural extensions of Python. When you import a library, what you get is a set of variables, functions, and classes, that augment and complement your “toolbox” of code. When using them, you have a certain set of expectations about how they behave. In my opinion, when it comes to Tensorflow, you should throw all that away. It’s fundamentally the wrong way to think about what Tensorflow is and how it interacts with the rest of your code.&lt;/p&gt;

&lt;p&gt;A metaphor for the relationship between Python and Tensorflow is the relationship between Javascript and HTML. Javascript is a fully-featured programming language that can do all sorts of wonderful things. HTML is a framework for representing a certain type of useful computational abstraction (in this case, content that can be rendered by a web browser). The role of Javascript in an interactive webpage is to assemble the HTML object that the browser sees, and then interact with it when necessary by updating it to new HTML.&lt;/p&gt;

&lt;p&gt;Similarly to HTML, Tensorflow is a framework for representing a certain type of computational abstraction (known as “computation graphs”). When we manipulate Tensorflow with Python, the first thing we do with our Python code is assemble the computation graph. Once that is done, the second thing we do is to interact with it (using Tensorflow’s “sessions”). But it’s important to keep in mind that the computation graph does not live inside of your variables; it lives in the global namespace. As Shakespeare once said: “All the RAM’s a stage, and all the variables are merely pointers.”&lt;/p&gt;

&lt;h3 id=&#34;first-key-abstraction-the-computation-graph&#34;&gt;First Key Abstraction: The Computation Graph&lt;/h3&gt;

&lt;p&gt;In browsing the Tensorflow documentation, you’ve probably found oblique references to “graphs” and “nodes”. If you’re a particularly savvy browser, you may have even discovered &lt;a href=&#34;https://www.tensorflow.org/programmers_guide/graphs&#34; target=&#34;_blank&#34;&gt;this page&lt;/a&gt;, which covers the content I’m about to explain in a much more accurate and technical fashion. This section is a high-level walkthrough that captures the important intuition, while sacrificing some technical details.&lt;/p&gt;

&lt;p&gt;So: what is a computation graph? Essentially, it’s a global data structure: a directed graph that captures instructions about how to calculate things.&lt;/p&gt;

&lt;p&gt;Let’s walk through an example of how to build one. In the following figures, the top half is the code we ran and its output, and the bottom half is the resulting computation graph.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig0.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Predictably, just importing Tensorflow does not give us an interesting computation graph. Just a lonely, empty global variable. But what about when we call a Tensorflow operation?&lt;/p&gt;

&lt;h6 id=&#34;code&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
print two_node
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Tensor(&amp;quot;Const:0&amp;quot;, shape=(), dtype=int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-1&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig1.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Would you look at that! We got ourselves a node. It contains the constant 2. Shocking, I know, coming from a function called &lt;code&gt;tf.constant&lt;/code&gt;. When we print the variable, we see that it returns a &lt;code&gt;tf.Tensor&lt;/code&gt; object, which is a pointer to the node that we just created. To emphasize this, here’s another example:&lt;/p&gt;

&lt;h6 id=&#34;code-1&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
another_two_node = tf.constant(2)
two_node = tf.constant(2)
tf.constant(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-2&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig2.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Every time we call &lt;code&gt;tf.constant&lt;/code&gt;, we create a new node in the graph. This is true even if the node is functionally identical to an existing node, even if we re-assign a node to the same variable, or  even if we don’t assign it to a variable at all.&lt;/p&gt;

&lt;p&gt;In contrast, if you make a new variable and set it equal to an existing node, you are just copying the pointer to that node and nothing is added to the graph:&lt;/p&gt;

&lt;h6 id=&#34;code-2&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
another_pointer_at_two_node = two_node
two_node = None
print two_node
print another_pointer_at_two_node
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-1&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;None
Tensor(&amp;quot;Const:0&amp;quot;, shape=(), dtype=int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-3&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig3.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Okay, let’s liven things up a bit:&lt;/p&gt;

&lt;h6 id=&#34;code-3&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node ## equivalent to tf.add(two_node, three_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-4&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig4.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Now we’re talking - that’s a bona-fide computational graph we got there! Notice that the &lt;code&gt;+&lt;/code&gt; operation is overloaded in Tensorflow, so adding two tensors together adds a node to the graph, even though it doesn’t seem like a Tensorflow operation on the surface.&lt;/p&gt;

&lt;p&gt;Okay, so &lt;code&gt;two_node&lt;/code&gt; points to a node containing 2, &lt;code&gt;three_node&lt;/code&gt; points to a node containing 3, and &lt;code&gt;sum_node&lt;/code&gt; points to a node containing…&lt;code&gt;+&lt;/code&gt;? What’s up with that? Shouldn’t it contain 5?&lt;/p&gt;

&lt;p&gt;As it turns out, no. Computational graphs contain only the steps of computation; they do not contain the results. At least…not yet!&lt;/p&gt;

&lt;h3 id=&#34;second-key-abstraction-the-session&#34;&gt;Second Key Abstraction: The Session&lt;/h3&gt;

&lt;p&gt;If there were March Madness for misunderstood TensorFlow abstractions, the session would be the #1 seed every year. It has that dubious honor due to being both unintuitively named and universally present &amp;ndash; nearly every Tensorflow program explicitly invokes &lt;code&gt;tf.Session()&lt;/code&gt; at least once.&lt;/p&gt;

&lt;p&gt;The role of the session is to handle the memory allocation and optimization that allows us to actually perform the computations specified by a graph. You can think of the computation graph as a “template” for the computations we want to do: it lays out all the steps. In order to make use of the graph, we also need to make a session, which allows us to actually do things; for example, going through the template node-by-node to allocate a bunch of memory for storing computation outputs. In order to do any computation with Tensorflow, you need both a graph and a session.&lt;/p&gt;

&lt;p&gt;The session contains a pointer to the global graph, which is constantly updated with pointers to all nodes. That means it doesn’t really matter whether you create the session before or after you create the nodes. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;After creating your session object, you can use &lt;code&gt;sess.run(node)&lt;/code&gt; to return the value of a node, and Tensorflow performs all computations necessary to determine that value.&lt;/p&gt;

&lt;h6 id=&#34;code-4&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
sess = tf.Session()
print sess.run(sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-2&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;5
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-5&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig4.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Wonderful! We can also pass a list, &lt;code&gt;sess.run([node1, node2,...])&lt;/code&gt;, and have it return multiple outputs:&lt;/p&gt;

&lt;h6 id=&#34;code-5&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
sess = tf.Session()
print sess.run([two_node, sum_node])
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-3&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[2, 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-6&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig4.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;In general, &lt;code&gt;sess.run()&lt;/code&gt; calls tend to be one of the biggest TensorFlow bottlenecks, so the fewer times you call it, the better. Whenever possible, return multiple items in a single &lt;code&gt;sess.run()&lt;/code&gt; call instead of making multiple calls.&lt;/p&gt;

&lt;h3 id=&#34;placeholders-feed-dict&#34;&gt;Placeholders &amp;amp; feed_dict&lt;/h3&gt;

&lt;p&gt;The computations we’ve done so far have been boring: there is no opportunity to pass in input, so they always output the same thing. A more worthwhile application might involve constructing a computation graph that takes in input, processes it in some (consistent) way, and returns an output.&lt;/p&gt;

&lt;p&gt;The most straightforward way to do this is with placeholders. A placeholder is a type of node that is designed to accept external input.&lt;/p&gt;

&lt;h6 id=&#34;code-6&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
input_placeholder = tf.placeholder(tf.int32)
sess = tf.Session()
print sess.run(input_placeholder)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-4&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Traceback (most recent call last):
...
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor &#39;Placeholder&#39; with dtype int32
	 [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=&amp;lt;unknown&amp;gt;, _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;]()]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-7&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig5.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;&amp;hellip;is a terrible example, since it throws an exception. Placeholders expect to be given a value. We didn’t supply one, so Tensorflow crashed.&lt;/p&gt;

&lt;p&gt;To provide a value, we use the feed_dict attribute of &lt;code&gt;sess.run()&lt;/code&gt;.&lt;/p&gt;

&lt;h6 id=&#34;code-7&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
input_placeholder = tf.placeholder(tf.int32)
sess = tf.Session()
print sess.run(input_placeholder, feed_dict={input_placeholder: 2})
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-5&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;2
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-8&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig5.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Much better. Notice the format of the dict passed into &lt;code&gt;feed_dict&lt;/code&gt;. The keys should be variables corresponding to placeholder nodes from the graph (which, as discussed earlier, really means &lt;em&gt;pointers&lt;/em&gt; to placeholder nodes in the graph). The corresponding values are the data elements to assign to each placeholder &amp;ndash; typically scalars or Numpy arrays.&lt;/p&gt;

&lt;h3 id=&#34;third-key-abstraction-computation-paths&#34;&gt;Third Key Abstraction: Computation Paths&lt;/h3&gt;

&lt;p&gt;Let’s try another example involving placeholders:&lt;/p&gt;

&lt;h6 id=&#34;code-8&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
input_placeholder = tf.placeholder(tf.int32)
three_node = tf.constant(3)
sum_node = input_placeholder + three_node
sess = tf.Session()
print sess.run(three_node)
print sess.run(sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-6&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;3
Traceback (most recent call last):
...
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor &#39;Placeholder_2&#39; with dtype int32
	 [[Node: Placeholder_2 = Placeholder[dtype=DT_INT32, shape=&amp;lt;unknown&amp;gt;, _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;]()]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-9&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig6.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Why does the second call to &lt;code&gt;sess.run()&lt;/code&gt; fail? And why does it raise an error related to &lt;code&gt;input_placeholder&lt;/code&gt;, even though we are not evaluating &lt;code&gt;input_placeholder&lt;/code&gt;? The answer lies in the final key Tensorflow abstraction: computation paths. Luckily, this one is very intuitive.&lt;/p&gt;

&lt;p&gt;When we call &lt;code&gt;sess.run()&lt;/code&gt; on a node that is dependent on other nodes in the graph, we need to compute the values of those nodes, too. And if those nodes have dependencies, we need to calculate those values (and so on and so on&amp;hellip;) until we reach the “top” of the computation graph where nodes have no predecessors.&lt;/p&gt;

&lt;p&gt;Consider the computation path of &lt;code&gt;sum_node&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig7.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;
&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig8.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;All three nodes need to be evaluated to compute the value of &lt;code&gt;sum_node&lt;/code&gt;. Crucially, this includes our un-filled placeholder and explains the exception!&lt;/p&gt;

&lt;p&gt;In contrast, consider the computation path of &lt;code&gt;three_node&lt;/code&gt;:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig9.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Due to the graph structure, we don’t need to compute all of the nodes in order to evaluate the one we want! Because we don’t need to evaluate &lt;code&gt;placeholder_node&lt;/code&gt; to evaluate &lt;code&gt;three_node&lt;/code&gt;, running &lt;code&gt;sess.run(three_node)&lt;/code&gt; doesn’t raise an exception.&lt;/p&gt;

&lt;p&gt;The fact that Tensorflow automatically routes computation only through nodes that are necessary is a huge strength of the framework. It saves a lot of runtime on calls if the graph is very big and has many nodes that are not necessary. It allows us to construct large, “multi-purpose” graphs, which use a single, shared set of core nodes to do different things depending on which computation path is taken. For almost every application, it’s important to think about &lt;code&gt;sess.run()&lt;/code&gt; calls in terms of the computation path taken.&lt;/p&gt;

&lt;h3 id=&#34;variables-side-effects&#34;&gt;Variables &amp;amp; Side Effects&lt;/h3&gt;

&lt;p&gt;So far, we’ve seen two types of “no-ancestor” nodes: &lt;code&gt;tf.constant&lt;/code&gt;, which is the same for every run, and &lt;code&gt;tf.placeholder&lt;/code&gt;, which is different for every run. There’s a third case that we often want to consider: a node which &lt;em&gt;generally&lt;/em&gt; has the same value between runs, but can also be updated to have a new value. That’s where variables come in.&lt;/p&gt;

&lt;p&gt;Understanding variables is essential to doing deep learning with Tensorflow, because the parameters of your model fall into this category. During training, you want to update your parameters at every step, via gradient descent; but during evaluation, you want to keep your parameters fixed, and pass a bunch of different test-set inputs into the model. More than likely, all of your model’s trainable parameters will be implemented as variables.&lt;/p&gt;

&lt;p&gt;To create variables, use &lt;code&gt;tf.get_variable()&lt;/code&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; The first two arguments to &lt;code&gt;tf.get_variable()&lt;/code&gt; are required; the rest are optional. They are &lt;code&gt;tf.get_variable(name, shape)&lt;/code&gt;. &lt;code&gt;name&lt;/code&gt; is a string which uniquely identifies this variable object. It must be unique relative to the global graph, so be careful to keep track of all names you have used to ensure there are no duplicates.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;code&gt;shape&lt;/code&gt; is an array of integers corresponding to the shape of a tensor; the syntax of this is intuitive &amp;ndash; just one integer per dimension, in order. For example, a 3x8 matrix would have shape &lt;code&gt;[3, 8]&lt;/code&gt;. To create a scalar, use an empty list as your shape: &lt;code&gt;[]&lt;/code&gt;.&lt;/p&gt;

&lt;h6 id=&#34;code-9&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
count_variable = tf.get_variable(&amp;quot;count&amp;quot;, [])
sess = tf.Session()
print sess.run(count_variable)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-7&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Traceback (most recent call last):
...
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value count
	 [[Node: _retval_count_0_0 = _Retval[T=DT_FLOAT, index=0, _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;](count)]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-10&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig10.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Alas, another exception. When a variable node is first created, it basically stores “null”, and any attempts to evaluate it will result in this exception. We can only evaluate a variable after putting a value into it first. There are two main ways to put a value into a variable: initializers and &lt;code&gt;tf.assign()&lt;/code&gt;. Let’s look at &lt;code&gt;tf.assign()&lt;/code&gt; first:&lt;/p&gt;

&lt;h6 id=&#34;code-10&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
count_variable = tf.get_variable(&amp;quot;count&amp;quot;, [])
zero_node = tf.constant(0.)
assign_node = tf.assign(count_variable, zero_node)
sess = tf.Session()
sess.run(assign_node)
print sess.run(count_variable)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-8&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;0
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-11&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig11.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;code&gt;tf.assign(target, value)&lt;/code&gt; is a node that has some unique properties compared to nodes we’ve seen so far:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Identity operation. &lt;code&gt;tf.assign(target, value)&lt;/code&gt; does not do any interesting computations, it is always just equal to &lt;code&gt;value&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Side effects. When computation “flows” through &lt;code&gt;assign_node&lt;/code&gt;, side effects happen to other things in the graph. In this case, the side effect is to replace the value of &lt;code&gt;count_variable&lt;/code&gt; with the value stored in &lt;code&gt;zero_node&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Non-dependent edges. Even though the &lt;code&gt;count_variable&lt;/code&gt; node and the &lt;code&gt;assign_node&lt;/code&gt; are connected in the graph, neither is dependent on the other. This means computation will not flow back through that edge when evaluating either node. However, &lt;code&gt;assign_node&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; dependent on &lt;code&gt;zero_node&lt;/code&gt;; it needs to know what to assign.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Side effect” nodes underpin most of the Tensorflow deep learning workflow, so make sure you really understand what’s going on here. When we call &lt;code&gt;sess.run(assign_node)&lt;/code&gt;, the computation path goes through &lt;code&gt;assign_node&lt;/code&gt; and &lt;code&gt;zero_node&lt;/code&gt;.&lt;/p&gt;

&lt;h6 id=&#34;graph-12&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig12.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;As computation flows through any node in the graph, it also enacts any side effects controlled by that node, shown in green. Due to the particular side effects of &lt;code&gt;tf.assign&lt;/code&gt;, the memory associated with &lt;code&gt;count_variable&lt;/code&gt; (which was previously “null”) is now permanently set to equal 0. This means that when we next call &lt;code&gt;sess.run(count_variable)&lt;/code&gt;, we don’t throw any exceptions. Instead, we get a value of 0. Success!&lt;/p&gt;

&lt;p&gt;Next, let’s look at initializers:&lt;/p&gt;

&lt;h6 id=&#34;code-11&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
const_init_node = tf.constant_initializer(0.)
count_variable = tf.get_variable(&amp;quot;count&amp;quot;, [], initializer=const_init_node)
sess = tf.Session()
print sess.run([count_variable])
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-9&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Traceback (most recent call last):
...
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value count
	 [[Node: _retval_count_0_0 = _Retval[T=DT_FLOAT, index=0, _device=&amp;quot;/job:localhost/replica:0/task:0/device:CPU:0&amp;quot;](count)]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-13&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig13.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Okay, what happened here? Why didn’t the initializer work?&lt;/p&gt;

&lt;p&gt;The answer lies in the split between sessions and graphs. We’ve set the &lt;code&gt;initializer&lt;/code&gt; property of &lt;code&gt;get_variable&lt;/code&gt; to point at our &lt;code&gt;const_init_node&lt;/code&gt;, but that just added a new connection between nodes in the graph. We haven’t done anything about the root of the exception: &lt;em&gt;the memory associated with the variable node&lt;/em&gt; (which is stored in the session, not the graph!) is still set to “null”. We need the session to tell the &lt;code&gt;const_init_node&lt;/code&gt; to actually update the variable.&lt;/p&gt;

&lt;h6 id=&#34;code-12&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
const_init_node = tf.constant_initializer(0.)
count_variable = tf.get_variable(&amp;quot;count&amp;quot;, [], initializer=const_init_node)
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
print sess.run(count_variable)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-10&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;0.
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-14&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig14.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;To do this, we added another, special node: &lt;code&gt;init = tf.global_variables_initializer()&lt;/code&gt;. Similarly to &lt;code&gt;tf.assign()&lt;/code&gt;, this is a node with side effects. In contrast to &lt;code&gt;tf.assign()&lt;/code&gt;, we don’t actually need to specify what its inputs are! &lt;code&gt;tf.global_variables_initializer()&lt;/code&gt; will look at the global graph at the moment of its creation and automatically add dependencies to every &lt;code&gt;tf.initializer&lt;/code&gt; in the graph. When we then evaluate it with &lt;code&gt;sess.run(init)&lt;/code&gt;, it goes to each of the initializers and tells them to do their thang, initializing the variables and allowing us to run &lt;code&gt;sess.run(count_variable)&lt;/code&gt; without an error.&lt;/p&gt;

&lt;h4 id=&#34;variable-sharing&#34;&gt;Variable Sharing&lt;/h4&gt;

&lt;p&gt;You may encounter Tensorflow code with variable sharing, which involves creating a scope and setting “reuse=True”. I strongly recommend that you don’t use this in your own code. If you want to use a single variable in multiple places, simply keep track of your pointer to that variable&amp;rsquo;s node programmatically, and re-use it when you need to. In other words, you should have only a single call of &lt;code&gt;tf.get_variable()&lt;/code&gt; for each parameter you intend to store in memory.&lt;/p&gt;

&lt;h3 id=&#34;optimizers&#34;&gt;Optimizers&lt;/h3&gt;

&lt;p&gt;At last: on to the actual deep learning! If you’re still with me, the remaining concepts should be extremely straightforward.&lt;/p&gt;

&lt;p&gt;In deep learning, the typical “inner loop” of training is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get an input and true_output&lt;/li&gt;
&lt;li&gt;Compute a “guess” based on the input and your parameters&lt;/li&gt;
&lt;li&gt;Compute a “loss” based on the difference between your guess and the true_output&lt;/li&gt;
&lt;li&gt;Update the parameters according to the gradient of the loss&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s put together a quick script for a toy linear regression problem:&lt;/p&gt;

&lt;h6 id=&#34;code-13&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf

### build the graph
## first set up the parameters
m = tf.get_variable(&amp;quot;m&amp;quot;, [], initializer=tf.constant_initializer(0.))
b = tf.get_variable(&amp;quot;b&amp;quot;, [], initializer=tf.constant_initializer(0.))
init = tf.global_variables_initializer()

## then set up the computations
input_placeholder = tf.placeholder(tf.float32)
output_placeholder = tf.placeholder(tf.float32)

x = input_placeholder
y = output_placeholder
y_guess = m * x + b

loss = tf.square(y - y_guess)

## finally, set up the optimizer and minimization node
optimizer = tf.train.GradientDescentOptimizer(1e-3)
train_op = optimizer.minimize(loss)

### start the session
sess = tf.Session()
sess.run(init)

### perform the training loop
import random

## set up problem
true_m = random.random()
true_b = random.random()

for update_i in range(100000):
  ## (1) get the input and output
  input_data = random.random()
  output_data = true_m * input_data + true_b

  ## (2), (3), and (4) all take place within a single call to sess.run()!
  _loss, _ = sess.run([loss, train_op], feed_dict={input_placeholder: input_data, output_placeholder: output_data})
  print update_i, _loss

### finally, print out the values we learned for our two variables
print &amp;quot;True parameters:     m=%.4f, b=%.4f&amp;quot; % (true_m, true_b)
print &amp;quot;Learned parameters:  m=%.4f, b=%.4f&amp;quot; % tuple(sess.run([m, b]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-11&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;0 2.3205383
1 0.5792742
2 1.55254
3 1.5733259
4 0.6435648
5 2.4061265
6 1.0746256
7 2.1998715
8 1.6775116
9 1.6462423
10 2.441034
...
99990 2.9878322e-12
99991 5.158629e-11
99992 4.53646e-11
99993 9.422685e-12
99994 3.991829e-11
99995 1.134115e-11
99996 4.9467985e-11
99997 1.3219648e-11
99998 5.684342e-14
99999 3.007017e-11
True parameters:     m=0.3519, b=0.3242
Learned parameters:  m=0.3519, b=0.3242
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the loss goes down to basically nothing, and we wind up with a really good estimate of the true parameters. Hopefully, the only part of the code that is new to you is this segment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## finally, set up the optimizer and minimization node
optimizer = tf.train.GradientDescentOptimizer(1e-3)
train_op = optimizer.minimize(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But, now that you have a good understanding of the concepts underlying Tensorflow, this code is easy to explain! The first line, &lt;code&gt;optimizer = tf.train.GradientDescentOptimizer(1e-3)&lt;/code&gt;, is not adding a node to the graph. It is simply creating a Python object that has useful helper functions. The second line, &lt;code&gt;train_op = optimizer.minimize(loss)&lt;/code&gt;, is adding a node to the graph, and storing a pointer to it in variable &lt;code&gt;train_op&lt;/code&gt;. The &lt;code&gt;train_op&lt;/code&gt; node has no output, but has a very complicated side effect:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;train_op&lt;/code&gt; traces back through the computation path of its input, &lt;code&gt;loss&lt;/code&gt;, looking for variable nodes. For each variable node it finds, it computes the gradient of the loss with respect to that variable. Then, it computes a new value for that variable: the current value minus the gradient times the learning rate. Finally, it performs an assign operation to update the value of the variable.&lt;/p&gt;

&lt;p&gt;So essentially, when we call &lt;code&gt;sess.run(train_op)&lt;/code&gt;, it does a step of gradient descent on all of our variables for us. Of course, we also need to fill in the input and output placeholders with our feed_dict, and we also want to print the loss, because it’s handy for debugging.&lt;/p&gt;

&lt;h3 id=&#34;debugging-with-tf-print&#34;&gt;Debugging with &lt;code&gt;tf.Print&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;As you start doing more complicated things with Tensorflow, you’re going to want to debug. In general, it’s quite hard to inspect what’s going on inside a computation graph. You can’t use a regular Python print statement, because you never have access to the values you want to print &amp;ndash; they are locked away inside the &lt;code&gt;sess.run()&lt;/code&gt; call. To elaborate, suppose you want to inspect an intermediate value of a computation. Before the &lt;code&gt;sess.run()&lt;/code&gt; call, the intermediate values do not exist yet. But when the &lt;code&gt;sess.run()&lt;/code&gt; call returns, the intermediate values are gone!&lt;/p&gt;

&lt;p&gt;Let’s look at a simple example.&lt;/p&gt;

&lt;h6 id=&#34;code-14&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
sess = tf.Session()
print sess.run(sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-12&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This lets us see our overall answer, 5. But what if we want to inspect the intermediate values, &lt;code&gt;two_node&lt;/code&gt; and &lt;code&gt;three_node&lt;/code&gt;? One way to inspect the intermediate values is to add a return argument to &lt;code&gt;sess.run()&lt;/code&gt; that points at each of the intermediate nodes you want to inspect, and then, after it has been returned, print it.&lt;/p&gt;

&lt;h6 id=&#34;code-15&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
sess = tf.Session()
answer, inspection = sess.run([sum_node, [two_node, three_node]])
print inspection
print answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-13&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[2, 3]
5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This often works well, but as code becomes more complex, it can be a bit awkward. A more convenient approach is to use a &lt;code&gt;tf.Print&lt;/code&gt; statement. Confusingly, &lt;code&gt;tf.Print&lt;/code&gt; is actually a type of Tensorflow node, which has both output and side effects! It has two required arguments: a node to copy, and a list of things to print. The “node to copy” can be any node in the graph; &lt;code&gt;tf.Print&lt;/code&gt; is an identity operation with respect to its “node to copy”, meaning that it outputs an exact copy of its input. But, it also prints all the current values in the “list of things to print” as a side effect.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h6 id=&#34;code-16&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
print_sum_node = tf.Print(sum_node, [two_node, three_node])
sess = tf.Session()
print sess.run(print_sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-14&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[2][3]
5
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-15&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig15.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;One important, somewhat-subtle point about &lt;code&gt;tf.Print&lt;/code&gt;: printing is a side effect. Like all other side effects, printing only occurs if the computation flows through the &lt;code&gt;tf.Print&lt;/code&gt; node. If the &lt;code&gt;tf.Print&lt;/code&gt; node is not in the path of the computation, nothing will print. In particular, even if the original node that your &lt;code&gt;tf.Print&lt;/code&gt; node is copying is on the computation path, the &lt;code&gt;tf.Print&lt;/code&gt; node itself might not be. Watch out for this issue! When it strikes (and it eventually will), it can be incredibly frustrating if you aren’t specifically looking for it. As a general rule, try to always create your &lt;code&gt;tf.Print&lt;/code&gt; node immediately after creating the node that it copies.&lt;/p&gt;

&lt;h6 id=&#34;code-17&#34;&gt;Code:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
two_node = tf.constant(2)
three_node = tf.constant(3)
sum_node = two_node + three_node
### this new copy of two_node is not on the computation path, so nothing prints!
print_two_node = tf.Print(two_node, [two_node, three_node, sum_node])
sess = tf.Session()
print sess.run(sum_node)
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;output-15&#34;&gt;Output:&lt;/h6&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;5
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;graph-16&#34;&gt;Graph:&lt;/h6&gt;

&lt;figure&gt;

&lt;img src=&#34;https://jacobbuckman.com/img/tfcp1/fig16.png&#34; width=&#34;300px&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://wookayin.github.io/tensorflow-talk-debugging/#1&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a great resource which provides additional practical debugging advice.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Hopefully this post helped you get a better intuition for what Tensorflow is, how it works, and how to use it. At the end of the day, the concepts presented here are fundamental to all Tensorflow programs, but this is only scratching the surface. In your Tensorflow adventures, you will likely encounter all sorts of other fun things that you want to use: conditionals, iteration, distributed Tensorflow, variable scopes, saving &amp;amp; loading models, multi-graph, multi-session, and multi-core, data-loader queues, and much more. Many of these topics I will cover in future posts. But if you build on the ideas you learned here with the official documentation, some code examples, and just a pinch of deep learning magic, I’m sure you’ll be able to figure it out!&lt;/p&gt;

&lt;p&gt;For more detail on how these abstractions are implemented in Tensorflow, and how to interact with them, take a look at my &lt;a href=&#34;https://jacobbuckman.com/post/graph-inspection/&#34; target=&#34;_blank&#34;&gt;post on inspecting computational graphs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Please give me feedback in the comments (or via email) if anything discussed in this guide was unclear. And if you enjoyed this post, let me know what I should cover next!&lt;/p&gt;

&lt;p&gt;Happy training!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Many thanks to Kathryn Rough, Katherine Lee, Sara Hooker, and Ludwig Schubert for all of their help and feedback when writing this post.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:0&#34;&gt;&lt;a href=&#34;https://docs.chainer.org/en/stable/guides/define_by_run.html&#34; target=&#34;_blank&#34;&gt;This page&lt;/a&gt; from the Chainer documentation describes the difference between define-and-run and define-by-run.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:0&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:1&#34;&gt;In general, I prefer to make sure I already have the entire graph in place when I create a session, and I follow that paradigm in my examples here. But you might see it done differently in other Tensorflow code.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Since the Tensorflow team is dedicated to backwards compatibility, there are several ways to create variables. In older code, it is common to also encounter the &lt;code&gt;tf.Variable()&lt;/code&gt; syntax, which serves the same purpose.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Name management can be made a bit easier with &lt;code&gt;tf.variable_scope()&lt;/code&gt;. I will cover scoping in more detail In a future post!
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Note that &lt;code&gt;tf.Print&lt;/code&gt; is not compatible with Colab or IPython notebooks; it prints to the standard output, which is not shown in the notebook. There are various solutions on StackOverflow.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is Generator Conditioning Causally Related to GAN Performance?</title>
      <link>https://jacobbuckman.com/publication/jacobean-gan/</link>
      <pubDate>Tue, 12 Jun 2018 12:27:43 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/publication/jacobean-gan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Lattice Language Models</title>
      <link>https://jacobbuckman.com/publication/nllm/</link>
      <pubDate>Sun, 03 Jun 2018 12:27:16 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/publication/nllm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Thermometer Encoding: One Hot Way to Resist Adversarial Examples</title>
      <link>https://jacobbuckman.com/publication/thermometer/</link>
      <pubDate>Wed, 25 Apr 2018 12:26:57 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/publication/thermometer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transition-Based Dependency Parsing with Heuristic Backtracking</title>
      <link>https://jacobbuckman.com/publication/heuristic-backtracking/</link>
      <pubDate>Fri, 25 Nov 2016 12:26:43 +0000</pubDate>
      
      <guid>https://jacobbuckman.com/publication/heuristic-backtracking/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
