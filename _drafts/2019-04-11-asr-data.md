---
layout: post
title: What do speech datasets sound like?
tags: ["speech recognition"]
mathjax: true
open_embed: true
published: false
---

Since I started working on speech recognition, I have only ever evaluated results using Word Error Rate (WER) metrics. Often, when some method performed differently on different datasets, senior researchers in my lab have told me it is "too noisy", or has "different channel characteristics". I have learnt about read speech and conversational speech, and how an ASR system needs to be robust to several variations in the utterance.

However, I have not once actually listened to these datasets to understand how complex the task is. I think many amateur researchers make this error starting out, since the data and recipes (in Kaldi, at least) are so nicely constructed, that you just tend to think of the speech data as a bunch of MFCC vectors. 

In this article, I put samples of several popular corpora used in ASR research, along with their characteristics and my impression of how they sound. The resource descriptions, in most cases, are taken from their respective [Kaldi](https://github.com/kaldi-asr/kaldi/tree/master/egs) recipes.

*Disclaimer*: Some of these datasets are proprietary, and I don't really know if sharing a small sample is permitted under the license. If you know about the particulars, please send me a mail at `draj@cs.jhu.edu` and I will update the post.

#### [Resource Management (RM)](https://catalog.ldc.upenn.edu/LDC93S3C)

Clean speech in a medium-vocabulary task consisting of commands to a (presumably imaginary) computer system. About 3 hours of training data. 

<p><audio><source src="/static/files/asr/rm.wav" type="audio/mp3"></audio></p>

Here is a transcription of the above sample: _SHOW THE GRIDLEY+S TRACK IN BRIGHT ORANGE WITH HORNE+S IN DIM RED_.

The sample sounds distinctly synthetic, and does not have any noise. As such, it is among the easiest datasets for ASR, and is often used for a *Hello World* equivalent for ASR systems.

#### [Babel](https://en.wikipedia.org/wiki/BABEL_Speech_Corpus)

The first version of Babel was recorded speech in 5 European languages (as mentioned on the Wiki), but the current version in Kaldi consists of 9 non-European languages like Assamese, Cantonese, Turkish, etc. Here is a sample from the Babel Bengali.

{{< audio src="asr/babel.wav" >}}

It sounds like conversational speech recorded at one end of a telephone conversation. There are several silences, which would correspond to the other person speaking (which is not recorded). There is a static noise in the background as well.

#### [Switchboard](https://www.isip.piconepress.com/projects/switchboard/)

This is telephonic speech in English released in 1997, collected as a 2-channel, 8kHz-sampled data. Here is a sample.

{{< audio src="asr/swbd.wav" >}}

It sounds similar to the Babel corpus, except that it is entirely in English and a little cleaner.


#### [Fisher English](https://catalog.ldc.upenn.edu/LDC2004S13)

This is conversational telephone speech collected as 2-channel, 8kHz-sampled data.  The data is similar to Switchboard but the transcription was mostly done in a "faster", lower-quality way. Here is a sample.

{{< audio src="asr/fe.wav" >}}

#### [Librispeech](http://www.openslr.org/12/)

LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned. There is a smaller version called [`mini_librispeech`](http://www.openslr.org/31/) created for the purpose of regression testing. Here is a sample.

{{< audio src="asr/libri.wav" >}}

The transcription for the above sample: _as you know and as i have given you proof i have the greatest admiration in the world for one whose work for humanity has won such universal recognition i hope that we shall both forget this unhappy morning and that you will give me an opportunity of rendering to you in person_

As is evident, this is read speech, and would probably be much easier to transcribe than the earlier datasets.

#### [TED-LIUM](http://www.openslr.org/7/)

The TED-LIUM corpus is English-language TED talks, with transcriptions, sampled at 16kHz. It contains about 118 hours of speech. Here is a sample.

{{< audio src="asr/tedlium.wav" >}}

We can see that the difficulty of transcription may be between a read speech and a conversational speech. There are very few silences, unlike the telephonic speech corpora, but there is still some background noise and reverberation since these are recorded talks.

#### [TIMIT](https://en.wikipedia.org/wiki/TIMIT)

The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Here is a sample.

{{< audio src="asr/timit.wav" >}}

#### [Wall Street Journal (WSJ)](https://catalog.ldc.upenn.edu/LDC93S6A)

This is a corpus of read sentences from the Wall Street Journal, recorded under clean conditions. The vocabulary is quite large. About 80 hours of training data. Here is a sample.

{{< audio src="asr/wsj.wav" >}}

As is clear, TIMIT and WSJ sound very similar, but the spoken content differs greatly.








