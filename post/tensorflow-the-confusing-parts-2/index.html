<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.55.5" />
  <meta name="author" content="Jacob Buckman">

  
  
  
  
    
  
  <meta name="description" content="This post is the second of a series; click here for the previous post, or here for a list of all posts in this series.
Naming and Scoping Naming Variables and Tensors As we discussed in Part 1, every time you call tf.get_variable(), you need to assign the variable a new, unique name. Actually, it goes deeper than that: every tensor in the graph gets a unique name too. The name can be accessed explicitly with the .">

  
  <link rel="alternate" hreflang="en-us" href="https://jacobbuckman.com/post/tensorflow-the-confusing-parts-2/">

  


  

  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7cMerriweather%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-62746966-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://jacobbuckman.com/index.xml" type="application/rss+xml" title="Buckman&#39;s Homepage">
  <link rel="feed" href="https://jacobbuckman.com/index.xml" type="application/rss+xml" title="Buckman&#39;s Homepage">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://jacobbuckman.com/post/tensorflow-the-confusing-parts-2/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@jacobmbuckman">
  <meta property="twitter:creator" content="@jacobmbuckman">
  
  <meta property="og:site_name" content="Buckman&#39;s Homepage">
  <meta property="og:url" content="https://jacobbuckman.com/post/tensorflow-the-confusing-parts-2/">
  <meta property="og:title" content="Tensorflow: The Confusing Parts (2) | Buckman&#39;s Homepage">
  <meta property="og:description" content="This post is the second of a series; click here for the previous post, or here for a list of all posts in this series.
Naming and Scoping Naming Variables and Tensors As we discussed in Part 1, every time you call tf.get_variable(), you need to assign the variable a new, unique name. Actually, it goes deeper than that: every tensor in the graph gets a unique name too. The name can be accessed explicitly with the .">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-09-17T00:30:19-04:00">
  
  <meta property="article:modified_time" content="2018-09-17T00:30:19-04:00">
  

  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
<script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#3f51b5",
          "text": "#fff"
        },
        "button": {
          "background": "#fff",
          "text": "#3f51b5"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://cookies.insites.com"
      }
    })});
</script>


  

  <title>Tensorflow: The Confusing Parts (2) | Buckman&#39;s Homepage</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Buckman&#39;s Homepage</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Tensorflow: The Confusing Parts (2)</h1>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Jacob Buckman</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2018-09-17 00:30:19 -0400 EDT" itemprop="datePublished">
    <time datetime="2018-09-17 00:30:19 -0400 EDT" itemprop="dateModified">
      Sep 17, 2018
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jacob Buckman">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="https://jacobbuckman.com/post/tensorflow-the-confusing-parts-2/#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://jacobbuckman.com/categories/tutorial/">Tutorial</a>, 
    
    <a href="https://jacobbuckman.com/categories/tftcp/">TFTCP</a>
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Tensorflow%3a%20The%20Confusing%20Parts%20%282%29&amp;url=https%3a%2f%2fjacobbuckman.com%2fpost%2ftensorflow-the-confusing-parts-2%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fjacobbuckman.com%2fpost%2ftensorflow-the-confusing-parts-2%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjacobbuckman.com%2fpost%2ftensorflow-the-confusing-parts-2%2f&amp;title=Tensorflow%3a%20The%20Confusing%20Parts%20%282%29"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fjacobbuckman.com%2fpost%2ftensorflow-the-confusing-parts-2%2f&amp;title=Tensorflow%3a%20The%20Confusing%20Parts%20%282%29"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Tensorflow%3a%20The%20Confusing%20Parts%20%282%29&amp;body=https%3a%2f%2fjacobbuckman.com%2fpost%2ftensorflow-the-confusing-parts-2%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p><em>This post is the second of a series; click <a href="https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/" target="_blank">here</a> for the previous post, or <a href="https://jacobbuckman.com/categories/tftcp/" target="_blank">here</a> for a list of all posts in this series.</em></p>

<h2 id="naming-and-scoping">Naming and Scoping</h2>

<h3 id="naming-variables-and-tensors">Naming Variables and Tensors</h3>

<p>As we discussed in Part 1, every time you call <code>tf.get_variable()</code>, you need to assign the variable a new, unique name. Actually, it goes deeper than that: every tensor in the graph gets a unique name too. The name can be accessed explicitly with the <code>.name</code> property of tensors, operations, and variables. For the vast majority of cases, the name will be created automatically for you; for example, a constant node will have the name <code>Const</code>, and as you create more of them, they will become <code>Const_1</code>, <code>Const_2</code>, etc.<sup class="footnote-ref" id="fnref:0"><a href="#fn:0">1</a></sup> You can also explicitly set the name of a node via the <code>name=</code> property, and the enumerative suffix will still be added automatically:</p>

<h6 id="code">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.constant(0.)
b = tf.constant(1.)
c = tf.constant(2., name=&quot;cool_const&quot;)
d = tf.constant(3., name=&quot;cool_const&quot;)
print a.name, b.name, c.name, d.name
</code></pre>

<h6 id="output">Output</h6>

<pre><code class="language-python">Const:0 Const_1:0 cool_const:0 cool_const_1:0
</code></pre>

<p>Explicitly naming nodes is nonessential, but can be very useful when debugging. Oftentimes, when your Tensorflow code crashes, the error trace will refer to a specific operation. If you have many operations of the same type, it can be tough to figure out which one is problematic. By explicitly naming each of your nodes, you can get much more informative error traces, and identify the issue more quickly.</p>

<h3 id="using-scopes">Using Scopes</h3>

<p>As your graph gets more complex, it becomes difficult to name everything by hand. Tensorflow provides the <code>tf.variable_scope</code> object, which makes it easier to organize your graphs by subdividing them into smaller chunks. By simply wrapping a segment of your graph creation code in a <code>with tf.variable_scope(scope_name):</code> statement, all nodes created will have their names automatically prefixed with the <code>scope_name</code> string. Additionally, these scopes stack; creating a scope within another will simply chain the prefixes together, delimited by a forward-slash.</p>

<h6 id="code-1">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.constant(0.)
b = tf.constant(1.)
with tf.variable_scope(&quot;first_scope&quot;):
  c = a + b
  d = tf.constant(2., name=&quot;cool_const&quot;)
  coef1 = tf.get_variable(&quot;coef&quot;, [], initializer=tf.constant_initializer(2.))
  with tf.variable_scope(&quot;second_scope&quot;):
    e = coef1 * d
    coef2 = tf.get_variable(&quot;coef&quot;, [], initializer=tf.constant_initializer(3.))
    f = tf.constant(1.)
    g = coef2 * f
    
print a.name, b.name
print c.name, d.name
print e.name, f.name, g.name
print coef1.name
print coef2.name

</code></pre>

<h6 id="output-1">Output</h6>

<pre><code class="language-python">Const:0 Const_1:0
first_scope/add:0 first_scope/cool_const:0
first_scope/second_scope/mul:0 first_scope/second_scope/Const:0 first_scope/second_scope/mul_1:0
first_scope/coef:0
first_scope/second_scope/coef:0
</code></pre>

<p>Notice that we were able to create two variables with the same name - <code>coef</code> - without any issues! This is because the scoping transformed the names into <code>first_scope/coef:0</code> and <code>first_scope/second_scope/coef:0</code>, which are distinct.</p>

<h2 id="saving-and-loading">Saving and Loading</h2>

<p>At its core, a trained neural network consists of two essential components:</p>

<ul>
<li>The <em>weights</em> of the network, which have been learned to optimize for some task</li>
<li>The <em>network graph</em>, which specifies how to actually use the weights to get results</li>
</ul>

<p>Tensorflow separates these two components, but it&rsquo;s clear that they need to be very tightly paired.
Weights are useless without a graph structure describing how to use them, and a graph with random weights is no good either.
In fact, even something as small as swapping two weight matrices is likely to totally break your model.
This often leads to frustration among beginner Tensorflow users; using a pre-trained model as a component of a neural network is a great way to speed up training, but can break things in a myriad of ways.</p>

<h3 id="saving-a-model">Saving A Model</h3>

<p>When working with only a single model, Tensorflow&rsquo;s built-in tools for saving and loading are straightforward to use: simply create a <code>tf.train.Saver()</code>.
Similarly to the <code>tf.train.Optimizer</code> family, a <code>tf.train.Saver</code> is not itself a node, but instead a higher-level class that performs useful functions on top of pre-existing graphs.
And, as you may have anticipated, the &lsquo;useful function&rsquo; of a <code>tf.train.Saver</code> is saving and loading the model.
Let&rsquo;s see it in action!</p>

<h6 id="code-2">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
b = tf.get_variable('b', [])
init = tf.global_variables_initializer()

saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, './tftcp.model')
</code></pre>

<h6 id="output-2">Output</h6>

<p>Four new files:</p>

<pre><code class="language-python">checkpoint
tftcp.model.data-00000-of-00001
tftcp.model.index
tftcp.model.meta
</code></pre>

<p>There&rsquo;s a lot of stuff to break down here.</p>

<p>First of all: Why does it output <em>four</em> files, when we only saved one model?
The information needed to recreate the model is divided among them.
If you want to copy or back up a model, make sure you bring all three of the files (the three prefixed by your filename).
Here&rsquo;s a quick description of each:</p>

<ul>
<li><code>tftcp.model.data-00000-of-00001</code> contains the weights of your model (the first bullet point from above). It&rsquo;s most likely the largest file here.</li>
<li><code>tftcp.model.meta</code> is the network structure of your model (the second bullet point from above). It contains all the information needed to re-create your graph.</li>
<li><code>tftcp.model.index</code> is an indexing structure linking the first two things. It says &ldquo;where in the data file do I find the parameters corresponding to this node?&rdquo;</li>
<li><code>checkpoint</code> is not actually needed to reconstruct your model, but if you save multiple versions of your model throughout a training run, it keeps track of everything.</li>
</ul>

<p>Secondly, why did I go through all the trouble of creating a <code>tf.Session</code> and <code>tf.global_variables_initializer</code> for this example?</p>

<p>Well, if we&rsquo;re going to save a model, we need to have something to save.
Recall that computations live in the graph, but values live in the session.
The <code>tf.train.Saver</code> can access the structure of the network through a global pointer to the graph.
But when we go to save the <em>values of the variables</em> (i.e. the weights of the network), we need to access a <code>tf.Session</code> to see what those values are; that&rsquo;s why <code>sess</code> is passed in as the first argument of the <code>save</code> function.
Additionally, attempting to save uninitialized variables will throw an error, because attempting to access the value of an uninitialized variable always throws an error.
So, we needed both a session and an initializer (or equivalent, e.g. <code>tf.assign</code>).</p>

<h3 id="loading-a-model">Loading A Model</h3>

<p>Now that we&rsquo;ve saved our model, let&rsquo;s load it back in.
The first step is to recreate the variables: we want variables with all the same names, shapes, and dtypes as we had when we saved it.
The second step is to create a <code>tf.train.Saver</code> just as before, and call the <code>restore</code> function.</p>

<h6 id="code-3">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
b = tf.get_variable('b', [])

saver = tf.train.Saver()
sess = tf.Session()
saver.restore(sess, './tftcp.model')
sess.run([a,b])
</code></pre>

<h6 id="output-3">Output</h6>

<pre><code class="language-python">[1.3106428, 0.6413864]
</code></pre>

<p>Note that we didn&rsquo;t need to initialize <code>a</code> or <code>b</code> before running them!
This is because the <code>restore</code> operation moves the values from our files into the session&rsquo;s variables.
Since the session no longer contains any null-valued variables, initialization is no longer needed.
(This can backfire if we aren&rsquo;t careful: running an init <em>after</em> a restore will override the loaded values with randomly-initialized ones.)</p>

<h3 id="choosing-your-variables">Choosing Your Variables</h3>

<p>When a <code>tf.train.Saver</code> is initialized, it looks at the current graph and gets the list of variables; this is permanently stored as the list of variables that that saver &ldquo;cares about&rdquo;.
We can inspect it with the <code>._var_list</code> property:</p>

<h6 id="code-4">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
b = tf.get_variable('b', [])
saver = tf.train.Saver()
c = tf.get_variable('c', [])
print saver._var_list
</code></pre>

<h6 id="output-4">Output</h6>

<pre><code class="language-python">[&lt;tf.Variable 'a:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'b:0' shape=() dtype=float32_ref&gt;]
</code></pre>

<p>Since <code>c</code> wasn&rsquo;t around at the time of our saver&rsquo;s creation, it does not get to be a part of the fun.
So in general, make sure that you already have all your variables created before creating a saver.</p>

<p>Of course, there are also some specific circumstances where you may actually want to only save a subset of your variables!
<code>tf.train.Saver</code> lets you pass the <code>var_list</code> when you create it to specify which subset of available variables you want it to keep track of.</p>

<h6 id="code-5">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
b = tf.get_variable('b', [])
c = tf.get_variable('c', [])
saver = tf.train.Saver(var_list=[a,b])
print saver._var_list
</code></pre>

<h6 id="output-5">Output</h6>

<pre><code class="language-python">[&lt;tf.Variable 'a:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'b:0' shape=() dtype=float32_ref&gt;]
</code></pre>

<h3 id="loading-modified-models">Loading Modified Models</h3>

<p>The examples above cover the &lsquo;perfect sphere in frictionless vacuum&rsquo; scenario of model-loading.
As long as you are saving and loading your own models, using your own code, without changing things in between, saving and loading is a breeze.
But in many cases, things are not so clean.
And in those cases, we need to get a little fancier.</p>

<p>Let&rsquo;s take a look at a couple of scenarios to illustrate the issues.
First, something that works without a problem.
What if we want to save a whole model, but we only want to load part of it?
(In the following code example, I run the two scripts in order.)</p>

<h6 id="code-6">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
b = tf.get_variable('b', [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, './tftcp.model')
</code></pre>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.restore(sess, './tftcp.model')
sess.run(a)
</code></pre>

<h6 id="output-6">Output</h6>

<pre><code class="language-python">1.1700551
</code></pre>

<p>Good, easy enough!
And yet, a failure case emerges when we have the reverse scenario: we want to load one model as a component of a larger model.</p>

<h6 id="code-7">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, './tftcp.model')
</code></pre>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
d = tf.get_variable('d', [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.restore(sess, './tftcp.model')
</code></pre>

<h6 id="output-7">Output</h6>

<pre><code class="language-python">Key d not found in checkpoint
         [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
</code></pre>

<p>We just wanted to load <code>a</code>, while ignoring the new variable <code>d</code>. And yet, we got an error, complaining that <code>d</code> was not present in the checkpoint!</p>

<p>A third scenario is where you want to load one model&rsquo;s parameters into a <em>different</em> model&rsquo;s computation graph.
This throws an error too, for obvious reasons: Tensorflow cannot possibly know where to put all those parameters you just loaded.
Luckily, there&rsquo;s a way to give it a hint.</p>

<p>Remember <code>var_list</code> from one section-header ago?
Well, it turns out to be a bit of a misnomer.
A better name might be &ldquo;var_list_or_dictionary_mapping_names_to_vars&rdquo;, but that&rsquo;s a mouthful, so I can sort of see why they stuck with the first bit.</p>

<p>Saving models is one of the key reasons that Tensorflow mandates globally-unique variable names.
In a saved-model-file, each saved variable&rsquo;s name is associated with its shape and value.
Loading it into a new computational graph is as easy as mapping the original-names of the variables you want to load to variables in your current model.
Here&rsquo;s an example:</p>

<h6 id="code-8">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, './tftcp.model')
</code></pre>

<pre><code class="language-python">import tensorflow as tf
d = tf.get_variable('d', [])
init = tf.global_variables_initializer()
saver = tf.train.Saver(var_list={'a': d})
sess = tf.Session()
sess.run(init)
saver.restore(sess, './tftcp.model')
sess.run(d)
</code></pre>

<h6 id="output-8">Output</h6>

<pre><code class="language-python">-0.9303965
</code></pre>

<p>This is the key mechanism by which you can combine models that do not have the exact same computational graph.
For example, perhaps you got a pre-trained language model off of the internet, and want to re-use the word embeddings.
Or, perhaps you changed the parameterization of your model in between training runs, and you want this new version to pick up where the old one left off; you don&rsquo;t want to have to re-train the whole thing from scratch.
In both of these cases, you would simply need to hand-make a dictionary mapping from the old variable names to the new variables.</p>

<p>A word of caution: it&rsquo;s very important to know <em>exactly</em> how the parameters you are loading are meant to be used.
If possible, you should use the exact code the original authors used to build their model, to ensure that that component of your computational graph is identical to how it looked during training.
If you need to re-implement, keep in mind that basically any change, no matter how minor, is likely to severely damage the performance of your pre-trained net.
Always benchmark your reimplementation against the original!</p>

<h4 id="inspecting-models">Inspecting Models</h4>

<p>If the model you want to load came from the internet - or from yourself, &gt;2 months ago - there&rsquo;s a good chance you won&rsquo;t <em>know</em> how the original variables were named.
To inspect saved models, use <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/framework/checkpoint_utils.py" target="_blank">these tools</a>, which come from the official Tensorflow repository.
For example:</p>

<h6 id="code-9">Code:</h6>

<pre><code class="language-python">import tensorflow as tf
a = tf.get_variable('a', [])
b = tf.get_variable('b', [10,20])
c = tf.get_variable('c', [])
init = tf.global_variables_initializer()
saver = tf.train.Saver()
sess = tf.Session()
sess.run(init)
saver.save(sess, './tftcp.model')
print tf.contrib.framework.list_variables('./tftcp.model')
</code></pre>

<h6 id="output-9">Output</h6>

<pre><code class="language-python">[('a', []), ('b', [10, 20]), ('c', [])]
</code></pre>

<p>With a little effort and a lot of head-scratching, it&rsquo;s usually possible to use these tools (in conjunction with the original codebase) to find the names of the variables you want.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Hopefully this post helped clear up the basics behind saving and loading Tensorflow models.
There are a few other advanced tricks, like automatic checkpointing and saving/restoring meta-graphs, that I may touch on in a future post; but in my experience, those use-cases are rare, especially for beginners.
As always, please let me know in the comments or via email if I got anything wrong, or there is anything important I missed.
Thanks for reading!</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:0">There will also be a suffix <code>:output_num</code> added to the tensor names. For now, that&rsquo;s always <code>:0</code>, since we are only using operations with a single output. See <a href="https://stackoverflow.com/questions/40925652/in-tensorflow-whats-the-meaning-of-0-in-a-variables-name" target="_blank">this StackOverflow question for more info</a>. Thanks Su Tang for pointing this out!
 <a class="footnote-return" href="#fnref:0"><sup>^</sup></a></li>
</ol>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://jacobbuckman.com/tags/tensorflow/">Tensorflow</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/graph-inspection/">More on Graph Inspection</a></li>
        
        <li><a href="/post/tensorflow-the-confusing-parts-1/">Tensorflow: The Confusing Parts (1)</a></li>
        
      </ul>
    </div>
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "jacobbuckman" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//jacobbuckman.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

